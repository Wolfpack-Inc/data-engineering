{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install tensorflow==1.13.1 keras==2.2.4 -q"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n",
      "/opt/conda/lib/python3.7/site-packages/tensorflow/python/framework/dtypes.py:526: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint8 = np.dtype([(\"qint8\", np.int8, 1)])\n",
      "/opt/conda/lib/python3.7/site-packages/tensorflow/python/framework/dtypes.py:527: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_quint8 = np.dtype([(\"quint8\", np.uint8, 1)])\n",
      "/opt/conda/lib/python3.7/site-packages/tensorflow/python/framework/dtypes.py:528: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint16 = np.dtype([(\"qint16\", np.int16, 1)])\n",
      "/opt/conda/lib/python3.7/site-packages/tensorflow/python/framework/dtypes.py:529: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_quint16 = np.dtype([(\"quint16\", np.uint16, 1)])\n",
      "/opt/conda/lib/python3.7/site-packages/tensorflow/python/framework/dtypes.py:530: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint32 = np.dtype([(\"qint32\", np.int32, 1)])\n",
      "/opt/conda/lib/python3.7/site-packages/tensorflow/python/framework/dtypes.py:535: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  np_resource = np.dtype([(\"resource\", np.ubyte, 1)])\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "os.environ['TF_CPP_MIN_LOG_LEVEL'] = '3' \n",
    "\n",
    "from pyspark.sql import SparkSession, DataFrame\n",
    "from pyspark.sql.functions import window, avg, count\n",
    "from pyspark.sql import types as T\n",
    "import pyspark.sql.functions as F\n",
    "from pyspark.sql.functions import udf\n",
    "from pyspark.sql.window import Window\n",
    "\n",
    "from pyspark.ml.linalg import Matrix, Matrices, Vectors, SparseVector, DenseVector, VectorUDT\n",
    "from pyspark.mllib.evaluation import RegressionMetrics\n",
    "from pyspark.ml.feature import VectorAssembler\n",
    "from pyspark.ml import Pipeline, Transformer, PipelineModel\n",
    "from pyspark.ml.regression import LinearRegression\n",
    "\n",
    "import numpy as np\n",
    "\n",
    "from keras.models import Sequential, load_model, model_from_json, Model\n",
    "from keras.layers import Dense, Dropout, Input, BatchNormalization, LSTM, Bidirectional\n",
    "from keras.callbacks import EarlyStopping, ModelCheckpoint, TensorBoard, ModelCheckpoint\n",
    "from keras import backend as K\n",
    "from keras import optimizers, regularizers"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Create the spark session"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "spark = (SparkSession\n",
    "         .builder\n",
    "         .appName(\"Streaming\")\n",
    "         .config('spark.jars.packages', 'org.mongodb.spark:mongo-spark-connector_2.11:2.4.1,org.apache.spark:spark-sql-kafka-0-10_2.11:2.4.4')\n",
    "         .getOrCreate())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Read our processed time windows from mongo"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------+-----------------+-------------------+-------------------+--------------------+\n",
      "|n_tweets|            price|          sentiment|          timestamp|              window|\n",
      "+--------+-----------------+-------------------+-------------------+--------------------+\n",
      "|      24|9186.630000000001|0.17804166666666665|2019-11-03 14:44:00|[2019-11-03 14:34...|\n",
      "|      73|9183.607500000002|0.24112465753424658|2019-11-03 14:46:00|[2019-11-03 14:36...|\n",
      "|     126|9182.878333333334|0.25534523809523824|2019-11-03 14:48:00|[2019-11-03 14:38...|\n",
      "|     184|         9181.015| 0.2538385869565217|2019-11-03 14:50:00|[2019-11-03 14:40...|\n",
      "|     257|         9184.389|0.21677548638132285|2019-11-03 14:52:00|[2019-11-03 14:42...|\n",
      "|     311|         9186.677|0.21157234726688087|2019-11-03 14:54:00|[2019-11-03 14:44...|\n",
      "|     312|         9189.179|0.19037852564102548|2019-11-03 14:56:00|[2019-11-03 14:46...|\n",
      "|     316|           9192.1|0.17361044303797465|2019-11-03 14:58:00|[2019-11-03 14:48...|\n",
      "|     313|         9195.474|0.16318370607028743|2019-11-03 15:00:00|[2019-11-03 14:50...|\n",
      "|     385|         9193.544|0.15191168831168814|2019-11-03 15:02:00|[2019-11-03 14:52...|\n",
      "|     429|         9190.258|0.13599184149184138|2019-11-03 15:04:00|[2019-11-03 14:54...|\n",
      "|     473|9186.289999999999| 0.1452315010570823|2019-11-03 15:06:00|[2019-11-03 14:56...|\n",
      "|     504|         9186.683|0.14681289682539672|2019-11-03 15:08:00|[2019-11-03 14:58...|\n",
      "|     558|9190.850999999999| 0.1321562724014335|2019-11-03 15:10:00|[2019-11-03 15:00...|\n",
      "|     512|         9195.039|0.14367148437499985|2019-11-03 15:12:00|[2019-11-03 15:02...|\n",
      "|     474|          9200.66|0.15276476793248936|2019-11-03 15:14:00|[2019-11-03 15:04...|\n",
      "|     462|9207.261999999999|0.13937878787878777|2019-11-03 15:16:00|[2019-11-03 15:06...|\n",
      "|     456|         9207.818| 0.1348307017543858|2019-11-03 15:18:00|[2019-11-03 15:08...|\n",
      "|     471|         9205.565|0.14266518046709115|2019-11-03 15:20:00|[2019-11-03 15:10...|\n",
      "|     490|9203.279999999999|0.13861673469387736|2019-11-03 15:22:00|[2019-11-03 15:12...|\n",
      "+--------+-----------------+-------------------+-------------------+--------------------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df = (spark\n",
    "         .read\n",
    "         .format(\"mongo\")\n",
    "         .option(\"spark.mongodb.input.uri\", \"mongodb://165.22.199.122/processed.internal\")\n",
    "         .load()\n",
    "         .drop('_id')\n",
    "         .orderBy('window.end'))\n",
    "\n",
    "df.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Create a transformer to calculate the price difference and generate the y labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "class LabelTransformer(Transformer):\n",
    "    \"\"\"\n",
    "    Custorm tranformer that the label that needs to be predicted\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self):\n",
    "        super(LabelTransformer, self).__init__()\n",
    "        \n",
    "    def _transform(self, df: DataFrame) -> DataFrame:        \n",
    "        # Define the window function\n",
    "        window = Window.partitionBy().orderBy('timestamp')\n",
    "\n",
    "        # Create a price lag of 1 window\n",
    "        df = df.withColumn('prev_price', F.lag(df.price).over(window))\n",
    "\n",
    "        # Calculate the price difference\n",
    "        df = df.withColumn('price_diff', df.price - df.prev_price)\n",
    "        \n",
    "        # Y label\n",
    "        df = df.withColumn('label', F.lag(df.price, -2).over(window))\n",
    "\n",
    "        # Drop the previous price column\n",
    "        df = df.drop('prev_price', 'window')\n",
    "        \n",
    "        # Drop all nan values (first price)\n",
    "        df = df.na.drop()\n",
    "\n",
    "        return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------+-----------------+-------------------+-------------------+-------------------+--------+\n",
      "|n_tweets|            price|          sentiment|          timestamp|         price_diff|   label|\n",
      "+--------+-----------------+-------------------+-------------------+-------------------+--------+\n",
      "|      73|9183.607500000002|0.24112465753424658|2019-11-03 14:46:00| -3.022499999999127|9181.015|\n",
      "|     126|9182.878333333334|0.25534523809523824|2019-11-03 14:48:00|-0.7291666666678793|9184.389|\n",
      "|     184|         9181.015| 0.2538385869565217|2019-11-03 14:50:00|-1.8633333333345945|9186.677|\n",
      "|     257|         9184.389|0.21677548638132285|2019-11-03 14:52:00| 3.3739999999997963|9189.179|\n",
      "|     311|         9186.677|0.21157234726688087|2019-11-03 14:54:00| 2.2880000000004657|  9192.1|\n",
      "+--------+-----------------+-------------------+-------------------+-------------------+--------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "label_transformer = LabelTransformer()\n",
    "df_label = label_transformer.transform(df)\n",
    "df_label.show(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Create a transformer to bring all the features to one array"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "class TimeTransformer(Transformer):\n",
    "    \"\"\"\n",
    "    A custom Transformer which transforms all values to timeseries. This is needed to input it into\n",
    "    the neural network\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, window_size, slide_size, feature_length):\n",
    "        super(TimeTransformer, self).__init__()\n",
    "        self.window_size = window_size\n",
    "        self.slide_size = slide_size\n",
    "        self.feature_length = feature_length\n",
    "\n",
    "    def _transform(self, df: DataFrame) -> DataFrame:\n",
    "        \n",
    "        # Create the timeseries. Window n minutes and collect the list of variables needed\n",
    "        df_window = (df\n",
    "             .groupBy(F.window(df.timestamp, self.window_size, self.slide_size))\n",
    "             .agg(\n",
    "                 F.collect_list('price'), \n",
    "                 F.collect_list('sentiment'), \n",
    "                 F.collect_list('n_tweets'),\n",
    "                 F.last('label').alias('label'),\n",
    "                 F.max('timestamp').alias('timestamp')))\n",
    "\n",
    "        # Concatenate all array columns\n",
    "        df_features = df_window.withColumn('features', \n",
    "                    F.concat(\n",
    "                        F.col('collect_list(price)'), \n",
    "                        F.col('collect_list(sentiment)'),\n",
    "                        F.col('collect_list(n_tweets)')))\n",
    "\n",
    "        # Make sure all the values are there\n",
    "        df_features = df_features.where(F.size(F.col('features')) == self.feature_length)\n",
    "        \n",
    "        # Dropped the left over array columns\n",
    "        df_features = df_features.drop(\n",
    "            'window', \n",
    "            'collect_list(price)', \n",
    "            'collect_list(sentiment)', \n",
    "            'collect_list(n_tweets)')\n",
    "\n",
    "        # Parse the features as vector instead of array (length need to be consistent)\n",
    "        list_to_vector_udf = F.udf(lambda l: Vectors.dense(l), VectorUDT())\n",
    "\n",
    "        df_features = df_features.select(\n",
    "            df_features[\"label\"], \n",
    "            df_features[\"features\"], \n",
    "#             list_to_vector_udf(df_features[\"features\"]).alias(\"features\")\n",
    "        )\n",
    "\n",
    "        return df_features.drop('timestamp')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----------------+--------------------+\n",
      "|            label|            features|\n",
      "+-----------------+--------------------+\n",
      "|9190.783000000001|[9193.535, 9191.2...|\n",
      "|         9177.135|[9181.14000000000...|\n",
      "|9166.708999999999|[9178.72499999999...|\n",
      "|          9205.12|[9188.31000000000...|\n",
      "|9219.877000000002|[9225.366, 9225.5...|\n",
      "|9289.094000000001|[9276.361, 9281.3...|\n",
      "|9507.947999999999|[9485.876, 9506.0...|\n",
      "|         9338.444|[9349.09800000000...|\n",
      "|9334.369999999999|[9323.552, 9326.7...|\n",
      "|         9334.596|[9334.103, 9332.8...|\n",
      "|          9363.09|[9361.298, 9362.4...|\n",
      "|9371.324999999999|[9385.72499999999...|\n",
      "|         9353.047|[9351.784, 9352.3...|\n",
      "|9354.309000000001|[9352.392, 9353.5...|\n",
      "|         9181.469|[9169.53799999999...|\n",
      "|9224.529999999999|[9223.59222222222...|\n",
      "|9012.176000000001|[9009.57000000000...|\n",
      "|         8773.946|[8788.408, 8787.9...|\n",
      "|          8835.76|[8824.52600000000...|\n",
      "|         8811.962|[8815.41499999999...|\n",
      "+-----------------+--------------------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "time_transformer = TimeTransformer(\n",
    "    window_size='6 minutes', \n",
    "    slide_size='2 minutes', \n",
    "    feature_length=9)\n",
    "\n",
    "df_features = time_transformer.transform(df_label)\n",
    "df_features.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Keras transformer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[('label', 'double'), ('features', 'array<float>')]\n",
      "+-----------------+--------------------+\n",
      "|            label|            features|\n",
      "+-----------------+--------------------+\n",
      "|9190.783000000001|[9193.535, 9191.2...|\n",
      "|         9177.135|[9181.14, 9180.02...|\n",
      "|9166.708999999999|[9178.725, 9173.7...|\n",
      "|          9205.12|[9188.31, 9193.10...|\n",
      "|9219.877000000002|[9225.366, 9225.5...|\n",
      "+-----------------+--------------------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df_features = df_features.withColumn('features', df_features.features.cast('array<float>'))\n",
    "print(df_features.dtypes)\n",
    "df_features.show(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From /opt/conda/lib/python3.7/site-packages/tensorflow/python/framework/op_def_library.py:263: colocate_with (from tensorflow.python.framework.ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Colocations handled automatically by placer.\n",
      "WARNING:tensorflow:From /opt/conda/lib/python3.7/site-packages/tensorflow/python/ops/math_ops.py:3066: to_int32 (from tensorflow.python.ops.math_ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Use tf.cast instead.\n",
      "WARNING:tensorflow:From /tmp/spark-78f71ac1-7a58-4546-8db8-93c16c9ac8c6/userFiles-b7bc5347-0989-479f-80a8-fb7aa520c116/databricks_spark-deep-learning-1.5.0-spark2.4-s_2.11.jar/sparkdl/graph/utils.py:220: convert_variables_to_constants (from tensorflow.python.framework.graph_util_impl) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Use tf.compat.v1.graph_util.convert_variables_to_constants\n",
      "WARNING:tensorflow:From /opt/conda/lib/python3.7/site-packages/tensorflow/python/framework/graph_util_impl.py:245: extract_sub_graph (from tensorflow.python.framework.graph_util_impl) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Use tf.compat.v1.graph_util.extract_sub_graph\n",
      "INFO:tensorflow:Froze 6 variables.\n",
      "INFO:tensorflow:Converted 6 variables to const ops.\n",
      "INFO:tensorflow:Froze 0 variables.\n",
      "INFO:tensorflow:Converted 0 variables to const ops.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Detected DoubleType columns in dataframe passed to transform(). In Deep Learning Pipelines 1.0 and above, DoubleType columns can only be fed to input tensors of type tf.float64. To feed dataframe data to tensors of other types (e.g. tf.float32, tf.int32, tf.int64), use the corresponding Spark SQL data types (FloatType, IntegerType, LongType).\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From /opt/conda/lib/python3.7/site-packages/tensorflow/python/tools/optimize_for_inference_lib.py:113: remove_training_nodes (from tensorflow.python.framework.graph_util_impl) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Use tf.compat.v1.graph_util.remove_training_nodes\n"
     ]
    }
   ],
   "source": [
    "keras_estimator = KerasTransformer(inputCol='features', outputCol='prediction', modelFile='models/keras_weights.hdf5')\n",
    "final_df = keras_estimator.transform(df_features)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "final_df = final_df.withColumn('prediction', final_df['prediction'].getItem(0))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------+-----------------+--------------------+\n",
      "|prediction|            label|            features|\n",
      "+----------+-----------------+--------------------+\n",
      "|  9188.564|9190.783000000001|[9193.535, 9191.2...|\n",
      "|  9174.693|         9177.135|[9181.14, 9180.02...|\n",
      "|  9169.327|9166.708999999999|[9178.725, 9173.7...|\n",
      "|  9194.783|          9205.12|[9188.31, 9193.10...|\n",
      "|  9225.717|9219.877000000002|[9225.366, 9225.5...|\n",
      "|  9295.478|9289.094000000001|[9276.361, 9281.3...|\n",
      "|  9500.383|9507.947999999999|[9485.876, 9506.0...|\n",
      "|   9341.67|         9338.444|[9349.098, 9345.0...|\n",
      "|  9334.622|9334.369999999999|[9323.552, 9326.7...|\n",
      "| 9336.3125|         9334.596|[9334.103, 9332.8...|\n",
      "|  9359.925|          9363.09|[9361.298, 9362.4...|\n",
      "|  9368.821|9371.324999999999|[9385.725, 9381.7...|\n",
      "|   9351.87|         9353.047|[9351.784, 9352.3...|\n",
      "|  9356.248|9354.309000000001|[9352.392, 9353.5...|\n",
      "|  9166.117|         9181.469|[9169.538, 9172.7...|\n",
      "|  9221.235|9224.529999999999|[9223.592, 9224.2...|\n",
      "|  9005.163|9012.176000000001|[9009.57, 9007.46...|\n",
      "|  8791.218|         8773.946|[8788.408, 8787.9...|\n",
      "|  8836.558|          8835.76|[8824.526, 8825.1...|\n",
      "|  8812.892|         8811.962|[8815.415, 8815.3...|\n",
      "+----------+-----------------+--------------------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "final_df.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Linear regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [],
   "source": [
    "lr_pipeline.write().overwrite().save('lr_pipeline')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lr_estimator = LinearRegression(maxIter=10, regParam=0.3, elasticNetParam=0.8)\n",
    "lr_pipeline = Pipeline(stages=[lr_estimator]).fit(df_features)\n",
    "\n",
    "# Save the pipeline\n",
    "lr_pipeline.write().overwrite().save('lr_pipeline')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----------------+--------------------+-----------------+\n",
      "|            label|            features|       prediction|\n",
      "+-----------------+--------------------+-----------------+\n",
      "|9190.783000000001|[9193.535,9191.23...|9190.584564115663|\n",
      "|         9177.135|[9181.14000000000...| 9179.07620051795|\n",
      "|9166.708999999999|[9178.72499999999...|9173.560828482081|\n",
      "|          9205.12|[9188.31000000000...|9191.914361492025|\n",
      "|9219.877000000002|[9225.366,9225.54...|9224.582859729486|\n",
      "|9289.094000000001|[9276.361,9281.37...|9281.463804854786|\n",
      "|9507.947999999999|[9485.876,9506.06...| 9497.13243990458|\n",
      "|         9338.444|[9349.09800000000...| 9344.04252864767|\n",
      "|9334.369999999999|[9323.552,9326.79...|9326.498846735196|\n",
      "|         9334.596|[9334.103,9332.85...|9332.112661362595|\n",
      "|          9363.09|[9361.298,9362.43...|9361.081552730007|\n",
      "|9371.324999999999|[9385.72499999999...|9380.393790586293|\n",
      "|         9353.047|[9351.784,9352.34...| 9352.19848674924|\n",
      "|9354.309000000001|[9352.392,9353.59...| 9353.42440421845|\n",
      "|         9181.469|[9169.53799999999...|9172.745414562216|\n",
      "|9224.529999999999|[9223.59222222222...| 9224.01931169643|\n",
      "|9012.176000000001|[9009.57000000000...|9007.391314628296|\n",
      "|         8773.946|[8788.408,8787.93...| 8787.84369117269|\n",
      "|          8835.76|[8824.52600000000...|8825.921957773367|\n",
      "|         8811.962|[8815.41499999999...|8814.929941492288|\n",
      "+-----------------+--------------------+-----------------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df_transformed = lr_pipeline.transform(df_features)\n",
    "df_transformed.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [],
   "source": [
    "pred_rdd = df_transformed.rdd.map(lambda p: (p.prediction, p.label)).cache()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "10.201432235528591"
      ]
     },
     "execution_count": 60,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "metrics = RegressionMetrics(pred_rdd)\n",
    "metrics.rootMeanSquaredError"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Twitter stream"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the timestamp format\n",
    "timestampFormat = \"dd-MM-yyyy HH:mm:ss\"\n",
    "\n",
    "# Create the schema of incoming data\n",
    "twitter_schema = T.StructType([\n",
    "    T.StructField('timestamp', T.TimestampType(), False),\n",
    "    T.StructField('text', T.StringType(), False),\n",
    "    T.StructField('sentiment', T.DoubleType(), False)\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Read kafka stream and subscribe to twitter topic\n",
    "twitter_stream = (spark.readStream\n",
    "          .format('kafka')\n",
    "          .option('kafka.bootstrap.servers', 'kafka:9092')\n",
    "          .option('startingOffsets', 'latest')\n",
    "          .option('subscribe', 'twitter')\n",
    "          .load()\n",
    "          .select(F.col(\"key\").cast(\"string\"), \\\n",
    "                  F.from_json(F.col(\"value\").cast(\"string\"), twitter_schema, \\\n",
    "                  { \"timestampFormat\": timestampFormat }).alias(\"value\")))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create streaming moving windows\n",
    "twitter_aggregation = (twitter_stream\n",
    "                     .select('value.*')\n",
    "                     .withWatermark('timestamp', '10 seconds')\n",
    "                     .groupBy(window('timestamp', '10 minutes', '2 minutes'))\n",
    "                     .agg(avg('sentiment').alias('sentiment'), count('timestamp').alias('n_tweets'))).select(F.col('window.end').alias('timestamp'), F.col('sentiment'), F.col('n_tweets'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<pyspark.sql.streaming.StreamingQuery at 0x7f509a5e42b0>"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Add the timestamp as key\n",
    "twitter_aggregation = twitter_aggregation.withColumn('key', F.col('timestamp'))\n",
    "\n",
    "# Send the data to kafka\n",
    "(twitter_aggregation\n",
    "    .selectExpr(\"CAST(key AS STRING) AS key\", \"to_json(struct(*)) AS value\")\n",
    "    .writeStream\n",
    "    .format(\"kafka\")\n",
    "    .option(\"kafka.bootstrap.servers\", \"kafka:9092\")\n",
    "    .option(\"topic\", \"twitter-agg\")\n",
    "    .option(\"checkpointLocation\", \"checkpoints/twitter-agg\")\n",
    "    .start())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Crypto stream"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create the schema of incoming data\n",
    "crypto_schema = T.StructType([\n",
    "    T.StructField('timestamp', T.TimestampType(), False),\n",
    "    T.StructField('price', T.DoubleType(), False)\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Read kafka stream and subscribe to crypto topic\n",
    "crypto_stream = (spark.readStream\n",
    "          .format('kafka')\n",
    "          .option('kafka.bootstrap.servers', 'kafka:9092')\n",
    "          .option('startingOffsets', 'latest')\n",
    "          .option('subscribe', 'crypto')\n",
    "          .load()\n",
    "          .select(F.col(\"key\").cast(\"string\"), \\\n",
    "                  F.from_json(F.col(\"value\").cast(\"string\"), crypto_schema, \\\n",
    "                  { \"timestampFormat\": timestampFormat }).alias(\"value\")))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create streaming moving windows\n",
    "crypto_aggregation = (crypto_stream\n",
    "                     .select('value.*')\n",
    "                     .withWatermark('timestamp', '10 seconds')\n",
    "                     .groupBy(window('timestamp', '10 minutes', '2 minutes'))\n",
    "                     .agg(avg('price').alias('price'))).select(F.col('window.end').alias('timestamp'), F.col('price'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<pyspark.sql.streaming.StreamingQuery at 0x7f509a5d0240>"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Add the timestamp as key\n",
    "crypto_aggregation = crypto_aggregation.withColumn('key', F.col('timestamp'))\n",
    "\n",
    "# Send the data to kafka\n",
    "(crypto_aggregation\n",
    "    .selectExpr(\"CAST(key AS STRING) AS key\", \"to_json(struct(*)) AS value\")\n",
    "    .writeStream\n",
    "    .format(\"kafka\")\n",
    "    .option(\"kafka.bootstrap.servers\", \"kafka:9092\")\n",
    "    .option(\"topic\", \"crypto-agg\")\n",
    "    .option(\"checkpointLocation\", \"checkpoints/crypto-agg\")\n",
    "    .start())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Read the crypto aggregation stream"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- timestamp: timestamp (nullable = true)\n",
      " |-- price: double (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Create the schema of incoming aggregated crypto data\n",
    "crypto_agg_schema = T.StructType([\n",
    "    T.StructField('timestamp', T.TimestampType(), False),\n",
    "    T.StructField('price', T.DoubleType(), False)\n",
    "])\n",
    "\n",
    "# Read the crypto aggregation stream\n",
    "crypto_agg_stream = ((spark.readStream\n",
    "          .format('kafka')\n",
    "          .option('kafka.bootstrap.servers', 'kafka:9092')\n",
    "          .option('startingOffsets', 'latest')\n",
    "          .option('subscribe', 'crypto-agg')\n",
    "          .load()\n",
    "          .select(\n",
    "              F.col(\"key\").cast(\"string\"), \n",
    "              F.from_json(F.col(\"value\").cast(\"string\"), crypto_agg_schema).alias(\"value\")))\n",
    "                     .select('value.*'))\n",
    "\n",
    "crypto_agg_stream.printSchema()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Read the twitter aggregation stream"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- timestamp: timestamp (nullable = true)\n",
      " |-- sentiment: double (nullable = true)\n",
      " |-- n_tweets: integer (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Create the schema of incoming aggregated crypto data\n",
    "twitter_agg_schema = T.StructType([\n",
    "    T.StructField('timestamp', T.TimestampType(), False),\n",
    "    T.StructField('sentiment', T.DoubleType(), False),\n",
    "    T.StructField('n_tweets', T.IntegerType(), False)\n",
    "])\n",
    "\n",
    "# Read the twitter aggregation stream\n",
    "twitter_agg_stream = ((spark.readStream\n",
    "          .format('kafka')\n",
    "          .option('kafka.bootstrap.servers', 'kafka:9092')\n",
    "          .option('startingOffsets', 'latest')\n",
    "          .option('subscribe', 'twitter-agg')\n",
    "          .load()\n",
    "          .select(\n",
    "              F.col(\"key\").cast(\"string\"), \n",
    "              F.from_json(F.col(\"value\").cast(\"string\"), twitter_agg_schema).alias(\"value\")))\n",
    "                     .select('value.*'))\n",
    "\n",
    "twitter_agg_stream.printSchema()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Join the two streams"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "merged_stream = (crypto_agg_stream\n",
    "                    .join(twitter_agg_stream, 'timestamp')\n",
    "                    .withWatermark('timestamp', '10 seconds'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "merged_df_stream = (merged_stream\n",
    "         .writeStream\n",
    "         .format(\"console\")\n",
    "         .start())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'message': 'Stopped', 'isDataAvailable': False, 'isTriggerActive': False}"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "merged_df_stream.stop()\n",
    "merged_df_stream.status"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "class TimeTransformer(Transformer):\n",
    "    \"\"\"\n",
    "    A custom Transformer which transforms all values to timeseries. This is needed to input it into\n",
    "    the neural network\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, window_size, slide_size, feature_length):\n",
    "        super(TimeTransformer, self).__init__()\n",
    "        self.window_size = window_size\n",
    "        self.slide_size = slide_size\n",
    "        self.feature_length = feature_length\n",
    "\n",
    "    def _transform(self, df: DataFrame) -> DataFrame:\n",
    "        \n",
    "        # Create the timeseries. Window 24 minutes and collect the list of variables needed\n",
    "        df_window = (df\n",
    "             .groupBy(F.window(df.timestamp, self.window_size, self.slide_size))\n",
    "             .agg(\n",
    "                 F.collect_list('price'), \n",
    "                 F.collect_list('sentiment'), \n",
    "                 F.collect_list('n_tweets'),\n",
    "                 F.max('timestamp').alias('timestamp')))\n",
    "\n",
    "        # Concatenate all array columns\n",
    "        df_features = df_window.withColumn('features', \n",
    "                    F.concat(\n",
    "                        F.col('collect_list(price)'), \n",
    "                        F.col('collect_list(sentiment)'),\n",
    "                        F.col('collect_list(n_tweets)')))\n",
    "\n",
    "        # Make sure all the values are there\n",
    "        df_features = df_features.where(F.size(F.col('features')) == self.feature_length)\n",
    "\n",
    "        df_features = df_features.select(\n",
    "            df_features[\"timestamp\"],\n",
    "            df_features[\"features\"])\n",
    "        \n",
    "        # Change the features type to float to be compatable with keras\n",
    "        df_features = df_features.withColumn('features', df_features.features.cast('array<float>'))\n",
    "        \n",
    "        # Add the time of the bitcoin price prediction\n",
    "        df_features = df_features.withColumn('pred_timestamp', (df_features.timestamp + F.expr('INTERVAL 4 MINUTES')))\n",
    "\n",
    "        return df_features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "time_transformer = TimeTransformer(\n",
    "    window_size='6 minutes', \n",
    "    slide_size='2 minutes', \n",
    "    feature_length=9)\n",
    "\n",
    "dfs_features = time_transformer.transform(merged_stream)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dfs_features.isStreaming"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "features_stream = (dfs_features\n",
    "         .writeStream\n",
    "         .format(\"console\")\n",
    "         .start())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'message': 'Stopped', 'isDataAvailable': False, 'isTriggerActive': False}"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "features_stream.stop()\n",
    "features_stream.status"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_model():\n",
    "    model = Sequential()\n",
    "    model.add(Dense(16, activation='relu', input_shape=(9,)))\n",
    "    \n",
    "    model.add(Dense(8, activation='relu'))\n",
    "        \n",
    "    model.add(Dense(1))\n",
    "    model.compile(optimizer='adam', loss='mean_squared_error')\n",
    "    \n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From /opt/conda/lib/python3.7/site-packages/tensorflow/python/framework/op_def_library.py:263: colocate_with (from tensorflow.python.framework.ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Colocations handled automatically by placer.\n"
     ]
    }
   ],
   "source": [
    "def load_trained_model():\n",
    "    model = build_model()\n",
    "    model.load_weights('models/keras_weights.hdf5')\n",
    "    \n",
    "    return model\n",
    "\n",
    "model = load_trained_model()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "@udf('float')\n",
    "def keras_predict(features):\n",
    "    prediction = model.predict(np.array([features]))\n",
    "    return float(prediction[0][0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "dfs_pred = dfs_features.withColumn('prediction', keras_predict(dfs_features['features']))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<bound method DataFrame.printSchema of DataFrame[timestamp: timestamp, features: array<float>, pred_timestamp: timestamp, prediction: float]>\n"
     ]
    }
   ],
   "source": [
    "print(dfs_pred.printSchema)\n",
    "pred_stream = dfs_pred.writeStream.format('console').start()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'pred_stream' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-64-d49dc6d96b10>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mpred_stream\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstop\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      2\u001b[0m \u001b[0mpred_stream\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstatus\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'pred_stream' is not defined"
     ]
    }
   ],
   "source": [
    "pred_stream.stop()\n",
    "pred_stream.status"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dfs_pred.isStreaming"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Write the predictions to kafka"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<pyspark.sql.streaming.StreamingQuery at 0x7f5098464438>"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Add the timestamp as key\n",
    "dfs_pred_final = dfs_pred.withColumn('key', F.col('pred_timestamp'))\n",
    "\n",
    "# Send the data to kafka\n",
    "(dfs_pred_final\n",
    "    .selectExpr(\"CAST(key AS STRING) AS key\", \"to_json(struct(*)) AS value\")\n",
    "    .writeStream\n",
    "    .format(\"kafka\")\n",
    "    .option(\"kafka.bootstrap.servers\", \"kafka:9092\")\n",
    "    .option(\"topic\", \"prediction\")\n",
    "    .option(\"checkpointLocation\", \"checkpoints/prediction\")\n",
    "    .start())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch 2 2 3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "6:45"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
