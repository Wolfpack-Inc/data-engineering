{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install tensorflow==1.13.1 keras==2.2.4 -q"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install 'numpy<1.17' -q"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "os.environ['TF_CPP_MIN_LOG_LEVEL'] = '3' \n",
    "\n",
    "from pyspark.sql import SparkSession, DataFrame\n",
    "from pyspark.sql import types as T\n",
    "from pyspark.sql import functions as F\n",
    "from pyspark.sql.functions import udf\n",
    "from pyspark.sql.window import Window\n",
    "\n",
    "from pyspark.ml import Pipeline, Transformer\n",
    "from pyspark.ml.feature import StandardScaler\n",
    "\n",
    "import numpy as np\n",
    "\n",
    "from keras.models import Sequential, load_model, model_from_json, Model\n",
    "from keras.layers import Dense, Dropout, Input, BatchNormalization, LSTM, Bidirectional\n",
    "from keras.callbacks import EarlyStopping, ModelCheckpoint, TensorBoard, ModelCheckpoint\n",
    "from keras import backend as K\n",
    "from keras import optimizers, regularizers"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Create the spark session"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "spark = (SparkSession\n",
    "         .builder\n",
    "         .appName(\"Streaming\")\n",
    "         .config('spark.jars.packages', 'org.apache.spark:spark-sql-kafka-0-10_2.11:2.4.4')\n",
    "         .config('spark.cores.max', '6')\n",
    "         .getOrCreate())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Twitter stream"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the timestamp format\n",
    "timestampFormat = \"dd-MM-yyyy HH:mm:ss\"\n",
    "\n",
    "# Create the schema of incoming data\n",
    "twitter_schema = T.StructType([\n",
    "    T.StructField('timestamp', T.TimestampType(), False),\n",
    "    T.StructField('text', T.StringType(), False),\n",
    "    T.StructField('sentiment', T.DoubleType(), False)\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Read kafka stream and subscribe to twitter topic\n",
    "twitter_stream = (spark.readStream\n",
    "          .format('kafka')\n",
    "          .option('kafka.bootstrap.servers', 'kafka:9092')\n",
    "          .option('startingOffsets', 'latest')\n",
    "          .option('subscribe', 'twitter')\n",
    "          .load()\n",
    "          .select(F.col(\"key\").cast(\"string\"), \\\n",
    "                  F.from_json(F.col(\"value\").cast(\"string\"), twitter_schema, \\\n",
    "                  { \"timestampFormat\": timestampFormat }).alias(\"value\")))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create streaming moving windows\n",
    "twitter_aggregation = (twitter_stream\n",
    "                     .select('value.*')\n",
    "                     .withWatermark('timestamp', '20 seconds')\n",
    "                     .groupBy(F.window('timestamp', '10 minutes', '2 minutes'))\n",
    "                     .agg(F.avg('sentiment').alias('sentiment'), F.count('timestamp').alias('n_tweets'))).select(F.col('window.end').alias('timestamp'), F.col('sentiment'), F.col('n_tweets'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Add the timestamp as key\n",
    "twitter_aggregation = twitter_aggregation.withColumn('key', F.col('timestamp'))\n",
    "\n",
    "# Send the data to kafka\n",
    "(twitter_aggregation\n",
    "    .selectExpr(\"CAST(key AS STRING) AS key\", \"to_json(struct(*)) AS value\")\n",
    "    .writeStream\n",
    "    .format(\"kafka\")\n",
    "    .option(\"kafka.bootstrap.servers\", \"kafka:9092\")\n",
    "    .option(\"topic\", \"twitter-agg\")\n",
    "    .option(\"checkpointLocation\", \"checkpoints/twitter-agg\")\n",
    "    .start());"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Crypto stream"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create the schema of incoming data\n",
    "crypto_schema = T.StructType([\n",
    "    T.StructField('timestamp', T.TimestampType(), False),\n",
    "    T.StructField('price', T.DoubleType(), False)\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Read kafka stream and subscribe to crypto topic\n",
    "crypto_stream = (spark.readStream\n",
    "          .format('kafka')\n",
    "          .option('kafka.bootstrap.servers', 'kafka:9092')\n",
    "          .option('startingOffsets', 'latest')\n",
    "          .option('subscribe', 'crypto')\n",
    "          .load()\n",
    "          .select(F.col(\"key\").cast(\"string\"), \\\n",
    "                  F.from_json(F.col(\"value\").cast(\"string\"), crypto_schema, \\\n",
    "                  { \"timestampFormat\": timestampFormat }).alias(\"value\")))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create streaming moving windows\n",
    "crypto_aggregation = (crypto_stream\n",
    "                     .select('value.*')\n",
    "                     .withWatermark('timestamp', '20 seconds')\n",
    "                     .groupBy(F.window('timestamp', '10 minutes', '2 minutes'))\n",
    "                     .agg(F.avg('price').alias('price'))).select(F.col('window.end').alias('timestamp'), F.col('price'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Add the timestamp as key\n",
    "crypto_aggregation = crypto_aggregation.withColumn('key', F.col('timestamp'))\n",
    "\n",
    "# Send the data to kafka\n",
    "(crypto_aggregation\n",
    "    .selectExpr(\"CAST(key AS STRING) AS key\", \"to_json(struct(*)) AS value\")\n",
    "    .writeStream\n",
    "    .format(\"kafka\")\n",
    "    .option(\"kafka.bootstrap.servers\", \"kafka:9092\")\n",
    "    .option(\"topic\", \"crypto-agg\")\n",
    "    .option(\"checkpointLocation\", \"checkpoints/crypto-agg\")\n",
    "    .start());"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Read the crypto aggregation stream"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- timestamp: timestamp (nullable = true)\n",
      " |-- price: double (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Create the schema of incoming aggregated crypto data\n",
    "crypto_agg_schema = T.StructType([\n",
    "    T.StructField('timestamp', T.TimestampType(), False),\n",
    "    T.StructField('price', T.DoubleType(), False)\n",
    "])\n",
    "\n",
    "# Read the crypto aggregation stream\n",
    "crypto_agg_stream = ((spark.readStream\n",
    "          .format('kafka')\n",
    "          .option('kafka.bootstrap.servers', 'kafka:9092')\n",
    "          .option('startingOffsets', 'earliest')\n",
    "          .option('subscribe', 'crypto-agg')\n",
    "          .load()\n",
    "          .select(\n",
    "              F.col(\"key\").cast(\"string\"), \n",
    "              F.from_json(F.col(\"value\").cast(\"string\"), crypto_agg_schema).alias(\"value\")))\n",
    "                     .select('value.*'))\n",
    "\n",
    "crypto_agg_stream.printSchema()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Read the twitter aggregation stream"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- timestamp: timestamp (nullable = true)\n",
      " |-- sentiment: double (nullable = true)\n",
      " |-- n_tweets: integer (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Create the schema of incoming aggregated crypto data\n",
    "twitter_agg_schema = T.StructType([\n",
    "    T.StructField('timestamp', T.TimestampType(), False),\n",
    "    T.StructField('sentiment', T.DoubleType(), False),\n",
    "    T.StructField('n_tweets', T.IntegerType(), False)\n",
    "])\n",
    "\n",
    "# Read the twitter aggregation stream\n",
    "twitter_agg_stream = ((spark.readStream\n",
    "          .format('kafka')\n",
    "          .option('kafka.bootstrap.servers', 'kafka:9092')\n",
    "          .option('startingOffsets', 'earliest')\n",
    "          .option('subscribe', 'twitter-agg')\n",
    "          .load()\n",
    "          .select(\n",
    "              F.col(\"key\").cast(\"string\"), \n",
    "              F.from_json(F.col(\"value\").cast(\"string\"), twitter_agg_schema).alias(\"value\")))\n",
    "                     .select('value.*'))\n",
    "\n",
    "twitter_agg_stream.printSchema()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Join the two streams"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "merged_stream = (crypto_agg_stream\n",
    "                    .join(twitter_agg_stream, 'timestamp')\n",
    "                    .withWatermark('timestamp', '10 seconds'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "merged_stream_ = (merged_stream\n",
    "         .writeStream\n",
    "         .format(\"console\")\n",
    "         .start())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "class StandardScaler(Transformer):\n",
    "    \"\"\"\n",
    "    A custom Transformer which standarizes all values in our dataframe. We need this to be custom\n",
    "    because of the timeseries nature of our data.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, price_mean, sentiment_mean, n_tweets_mean, price_std, sentiment_std, n_tweets_std):\n",
    "        super(StandardScaler, self).__init__()\n",
    "    \n",
    "        self.price_mean     = price_mean\n",
    "        self.sentiment_mean = sentiment_mean\n",
    "        self.n_tweets_mean  = n_tweets_mean\n",
    "        self.price_std      = price_std\n",
    "        self.sentiment_std  = sentiment_std\n",
    "        self.n_tweets_std   = n_tweets_std\n",
    "\n",
    "    def _transform(self, df: DataFrame) -> DataFrame:\n",
    "        df = df.withColumn('price', (F.col('price') - self.price_mean) / self.price_std)\n",
    "        df = df.withColumn('sentiment', (F.col('sentiment') - self.sentiment_mean) / self.sentiment_std)\n",
    "        df = df.withColumn('n_tweets', (F.col('n_tweets') - self.n_tweets_mean) / self.n_tweets_std)\n",
    "        \n",
    "        return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "time_transformer = StandardScaler(\n",
    "        price_mean     = 9148.09815,\n",
    "        sentiment_mean = 0.171638227,\n",
    "        n_tweets_mean  = 406.556004,\n",
    "        price_std      = 230.07774359985365,\n",
    "        sentiment_std  = 0.040592431560575425,\n",
    "        n_tweets_std   = 95.89927361560149\n",
    ")\n",
    "\n",
    "dfs_standard = time_transformer.transform(merged_stream)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "class TimeTransformer(Transformer):\n",
    "    \"\"\"\n",
    "    A custom Transformer which transforms all values to timeseries. \n",
    "    This is needed to input it into the neural network\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, window_size, slide_size, feature_length):\n",
    "        super(TimeTransformer, self).__init__()\n",
    "        \n",
    "        self.window_size    = window_size\n",
    "        self.slide_size     = slide_size\n",
    "        self.feature_length = feature_length\n",
    "\n",
    "    def _transform(self, df: DataFrame) -> DataFrame:\n",
    "        \n",
    "        # Create the timeseries. Window and collect the list of variables needed\n",
    "        df_window = (df\n",
    "             .groupBy(F.window(df.timestamp, self.window_size, self.slide_size))\n",
    "             .agg(\n",
    "                 F.collect_list('price'), \n",
    "                 F.collect_list('sentiment'), \n",
    "                 F.collect_list('n_tweets'),\n",
    "                 F.max('timestamp').alias('timestamp')))\n",
    "\n",
    "        # Concatenate all array columns\n",
    "        df_features = df_window.withColumn('features', \n",
    "                    F.concat(\n",
    "                        F.col('collect_list(price)'), \n",
    "                        F.col('collect_list(sentiment)'),\n",
    "                        F.col('collect_list(n_tweets)')))\n",
    "\n",
    "        # Make sure all the features are the correct length\n",
    "        df_features = df_features.where(F.size(F.col('features')) == self.feature_length)\n",
    "\n",
    "        # Just select the timestamp and features\n",
    "        df_features = df_features.select(\n",
    "            df_features[\"timestamp\"],\n",
    "            df_features[\"features\"])\n",
    "        \n",
    "        # Change the features type to float to be compatable with keras\n",
    "        df_features = df_features.withColumn('features', df_features.features.cast('array<float>'))\n",
    "        \n",
    "        # Add the time of the bitcoin price prediction\n",
    "        df_features = df_features.withColumn('pred_timestamp', (df_features.timestamp + F.expr('INTERVAL 4 MINUTES')))\n",
    "\n",
    "        return df_features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "time_transformer = TimeTransformer(\n",
    "    window_size='6 minutes', \n",
    "    slide_size='2 minutes', \n",
    "    feature_length=9)\n",
    "\n",
    "dfs_features = time_transformer.transform(dfs_standard)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "features_stream = (dfs_features\n",
    "         .writeStream\n",
    "         .format(\"console\")\n",
    "         .start())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "# features_stream.stop()\n",
    "# features_stream.status"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Building and predicting using the keras model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_model():\n",
    "    \"\"\"\n",
    "    Function that creates a keras model\n",
    "    \"\"\"\n",
    "    model = Sequential()\n",
    "    \n",
    "    model.add(Dense(16, activation='relu', input_shape=(9,)))\n",
    "    model.add(Dense(8, activation='relu'))\n",
    "    model.add(Dense(1))\n",
    "    \n",
    "    model.compile(optimizer='adam', loss='mean_squared_error')\n",
    "    \n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From /opt/conda/lib/python3.7/site-packages/tensorflow/python/framework/op_def_library.py:263: colocate_with (from tensorflow.python.framework.ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Colocations handled automatically by placer.\n"
     ]
    }
   ],
   "source": [
    "def load_trained_model():\n",
    "    \"\"\"\n",
    "    Function that loads the stored weights into the model\n",
    "    \"\"\"\n",
    "    model = build_model()\n",
    "    model.load_weights('models/keras_weights.hdf5')\n",
    "    \n",
    "    return model\n",
    "\n",
    "model = load_trained_model()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "@udf('float')\n",
    "def keras_predict(features):\n",
    "    \"\"\"\n",
    "    User defined function that perform the actual prediction on the stream\n",
    "    \"\"\"\n",
    "    prediction = model.predict(np.array([features]))\n",
    "    return float(prediction[0][0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "dfs_pred = dfs_features.withColumn('prediction', keras_predict(dfs_features['features']))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<bound method DataFrame.printSchema of DataFrame[timestamp: timestamp, features: array<float>, pred_timestamp: timestamp, prediction: float]>\n"
     ]
    }
   ],
   "source": [
    "print(dfs_pred.printSchema)\n",
    "pred_stream = dfs_pred.writeStream.format('console').start()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "# pred_stream.stop()\n",
    "# pred_stream.status"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dfs_pred.isStreaming"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Write the predictions to kafka"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Only select prediction timestamp and prediction\n",
    "dfs_pred = dfs_pred.select(dfs_pred['pred_timestamp'], dfs_pred['prediction'])\n",
    "\n",
    "# Rename the timestamp\n",
    "dfs_pred = dfs_pred.withColumn('timestamp', F.col('pred_timestamp'))\n",
    "\n",
    "# Add the timestamp as key\n",
    "dfs_pred_final = dfs_pred.withColumn('key', F.col('timestamp'))\n",
    "\n",
    "# Transform the timestamp our standard format\n",
    "dfs_pred_final = dfs_pred_final.withColumn(\n",
    "    'timestamp', \n",
    "    F.from_unixtime(F.unix_timestamp(dfs_pred_final.timestamp), timestampFormat))\n",
    "\n",
    "# Send the data to kafka\n",
    "(dfs_pred_final\n",
    "    .selectExpr(\"CAST(key AS STRING) AS key\", \"to_json(struct(timestamp, prediction)) AS value\")\n",
    "    .writeStream\n",
    "    .format(\"kafka\")\n",
    "    .option(\"kafka.bootstrap.servers\", \"kafka:9092\")\n",
    "    .option(\"topic\", \"prediction\")\n",
    "    .option(\"checkpointLocation\", \"checkpoints/prediction\")\n",
    "    .start());"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
