{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Testing application"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "os.environ['PYSPARK_SUBMIT_ARGS'] = '--packages org.apache.spark:spark-sql-kafka-0-10_2.11:2.4.4 pyspark-shell'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql.functions import explode, window, max, min, avg\n",
    "from pyspark.sql.functions import split\n",
    "from pyspark.sql.types import StringType, FloatType, StructType, StructField, IntegerType, TimestampType\n",
    "from pyspark.sql.functions import from_json, col, to_timestamp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a spark session\n",
    "spark = SparkSession.builder.appName(\"Twitter\").getOrCreate()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the timestamp format\n",
    "timestampFormat = \"E MMM dd HH:mm:ss +0000 yyyy\"\n",
    "\n",
    "# Define the schema of the incoming tweets\n",
    "schema = (StructType()\n",
    "  .add('created_at', TimestampType())\n",
    "  .add('id_str', StringType())\n",
    "  .add('text', StringType()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- key: string (nullable = true)\n",
      " |-- value: struct (nullable = true)\n",
      " |    |-- created_at: timestamp (nullable = true)\n",
      " |    |-- id_str: string (nullable = true)\n",
      " |    |-- text: string (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Read kafka stream and subscribe to twitter topic\n",
    "df = (spark.readStream\n",
    "          .format('kafka')\n",
    "          .option('kafka.bootstrap.servers', 'kafka-1:9092')\n",
    "          .option('startingOffsets', 'latest')\n",
    "          .option('subscribe', 'twitter')\n",
    "          .load()\n",
    "          .select(col(\"key\").cast(\"string\"), \\\n",
    "                  from_json(col(\"value\").cast(\"string\"), schema, \\\n",
    "                  { \"timestampFormat\": timestampFormat }).alias(\"value\")))\n",
    "\n",
    "df.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- created_at: timestamp (nullable = true)\n",
      " |-- id_str: string (nullable = true)\n",
      " |-- text: string (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "tweets = df.select(\"value.*\")\n",
    "tweets.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "raw_df = (tweets\n",
    "         .writeStream\n",
    "         .queryName(\"tweets\")\n",
    "         .format(\"memory\")\n",
    "         .start())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------------------+-------------------+--------------------+\n",
      "|         created_at|             id_str|                text|\n",
      "+-------------------+-------------------+--------------------+\n",
      "|2019-10-23 15:54:51|1187034660943073281|RT @SebGorka: The...|\n",
      "|2019-10-23 15:54:50|1187034655351922695|Thank you Jill!! ...|\n",
      "|2019-10-23 15:54:49|1187034649941307397|RT @sweetnpeachie...|\n",
      "|2019-10-23 15:54:49|1187034649006088192|Heat basketball t...|\n",
      "|2019-10-23 15:54:45|1187034632643956737|Khimky Podmoskovi...|\n",
      "|2019-10-23 15:54:44|1187034630391681026|RT @go_seahorses:...|\n",
      "|2019-10-23 15:54:44|1187034629582155776|Three things to w...|\n",
      "|2019-10-23 15:54:42|1187034622615588865|RT @ParkPirates: ...|\n",
      "|2019-10-23 15:54:42|1187034621260816385|Whoop, whoop, Go ...|\n",
      "|2019-10-23 15:54:41|1187034618958110721|RT @karlita_papit...|\n",
      "|2019-10-23 15:54:41|1187034616252768256|Oh, basketball is...|\n",
      "|2019-10-23 15:54:41|1187034616210714625|RT @Reuters: Shaq...|\n",
      "|2019-10-23 15:54:39|1187034607427809280|                  üèÄ|\n",
      "|2019-10-23 15:54:39|1187034611026554880|             Lmfaooo|\n",
      "|2019-10-23 15:54:38|1187034604076765184|RT @nctmarkarchiv...|\n",
      "|2019-10-23 15:54:37|1187034600884842496|RT @LC_Bird_Hoops...|\n",
      "|2019-10-23 15:54:36|1187034598103928837|RT @arturodraws: ...|\n",
      "|2019-10-23 15:54:36|1187034597529391104|RT @retrokidshoop...|\n",
      "|2019-10-23 15:54:36|1187034597688795137|RT @badgallrayray...|\n",
      "|2019-10-23 15:54:35|1187034593062354946|CSKA II vs Samara...|\n",
      "+-------------------+-------------------+--------------------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "raw = spark.sql(\"select * from tweets order by created_at desc\")\n",
    "raw.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get the count of tweets per 10 seconds\n",
    "tweet_count_df = tweets.groupBy(window(tweets.created_at, '10 seconds')).count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [],
   "source": [
    "query = (tweet_count_df\n",
    "        .writeStream\n",
    "        .format(\"memory\")\n",
    "        .queryName(\"window_count\")\n",
    "        .outputMode(\"complete\")\n",
    "        .start())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------+-----+\n",
      "|              window|count|\n",
      "+--------------------+-----+\n",
      "|[2019-10-23 16:05...|   21|\n",
      "|[2019-10-23 16:05...|   15|\n",
      "|[2019-10-23 16:05...|   11|\n",
      "|[2019-10-23 16:05...|   23|\n",
      "|[2019-10-23 16:04...|   15|\n",
      "|[2019-10-23 16:04...|   14|\n",
      "|[2019-10-23 16:04...|   16|\n",
      "|[2019-10-23 16:04...|   20|\n",
      "|[2019-10-23 16:04...|   15|\n",
      "|[2019-10-23 16:04...|   17|\n",
      "|[2019-10-23 16:03...|   13|\n",
      "|[2019-10-23 16:03...|   10|\n",
      "|[2019-10-23 16:03...|   15|\n",
      "|[2019-10-23 16:03...|   12|\n",
      "|[2019-10-23 16:03...|    5|\n",
      "|[2019-10-23 16:03...|   13|\n",
      "|[2019-10-23 16:02...|   13|\n",
      "|[2019-10-23 16:02...|   23|\n",
      "|[2019-10-23 16:02...|   22|\n",
      "|[2019-10-23 16:02...|   13|\n",
      "+--------------------+-----+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "raw = spark.sql(\"select * from window_count order by window desc\")\n",
    "raw.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
