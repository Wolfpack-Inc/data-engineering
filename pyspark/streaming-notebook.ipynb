{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Testing application"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "os.environ['PYSPARK_SUBMIT_ARGS'] = '--packages org.apache.spark:spark-sql-kafka-0-10_2.11:2.4.4 pyspark-shell'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql.functions import explode, window, max, min, avg\n",
    "from pyspark.sql.functions import split\n",
    "from pyspark.sql.types import StringType, FloatType, StructType, StructField, IntegerType, TimestampType\n",
    "from pyspark.sql.functions import from_json, col, to_timestamp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a spark session\n",
    "spark = SparkSession.builder.appName(\"Twitter\").getOrCreate()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the timestamp format\n",
    "timestampFormat = \"E MMM dd HH:mm:ss +0000 yyyy\"\n",
    "\n",
    "# Define the schema of the incoming tweets\n",
    "schema = (StructType()\n",
    "  .add('created_at', TimestampType())\n",
    "  .add('id_str', StringType())\n",
    "  .add('text', StringType()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- key: string (nullable = true)\n",
      " |-- value: struct (nullable = true)\n",
      " |    |-- created_at: timestamp (nullable = true)\n",
      " |    |-- id_str: string (nullable = true)\n",
      " |    |-- text: string (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Read kafka stream and subscribe to twitter topic\n",
    "df = (spark.readStream\n",
    "          .format('kafka')\n",
    "          .option('kafka.bootstrap.servers', 'kafka-1:9092')\n",
    "          .option('startingOffsets', 'latest')\n",
    "          .option('subscribe', 'twitter')\n",
    "          .load()\n",
    "          .select(col(\"key\").cast(\"string\"), \\\n",
    "                  from_json(col(\"value\").cast(\"string\"), schema, \\\n",
    "                  { \"timestampFormat\": timestampFormat }).alias(\"value\")))\n",
    "\n",
    "df.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- created_at: timestamp (nullable = true)\n",
      " |-- id_str: string (nullable = true)\n",
      " |-- text: string (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "tweets = df.select(\"value.*\")\n",
    "tweets.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "raw_df = (tweets\n",
    "         .writeStream\n",
    "         .queryName(\"tweets\")\n",
    "         .format(\"memory\")\n",
    "         .start())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------------------+-------------------+------------------------------------+\n",
      "|         created_at|             id_str|                                text|\n",
      "+-------------------+-------------------+------------------------------------+\n",
      "|2019-10-29 14:42:01|1189190655601336320|                UP is the most ha...|\n",
      "|2019-10-29 14:42:01|1189190659485270017|                RT @alfuckuhard: ...|\n",
      "|2019-10-29 14:42:01|1189190655852986370|                RT @steelers1288:...|\n",
      "|2019-10-29 14:42:01|1189190659111960578|                RT @renminrise: j...|\n",
      "|2019-10-29 14:42:00|1189190652824698880|                RT @MickstapeShow...|\n",
      "|2019-10-29 14:41:59|1189190648307408902|                RT @CentristDan: ...|\n",
      "|2019-10-29 14:41:58|1189190643668488194|                .@richarddeitsch ...|\n",
      "|2019-10-29 14:41:57|1189190640317255681|                RT @CultureCentra...|\n",
      "|2019-10-29 14:41:56|1189190638228529152|                Today's MSU baske...|\n",
      "|2019-10-29 14:41:55|1189190631563771905|                Jay Wright’s summ...|\n",
      "|2019-10-29 14:41:55|1189190632255848448|男子バスケ日本代表ヨーロッパ遠征 ...|\n",
      "|2019-10-29 14:41:53|1189190623741370368|                RT @_monicaaa__: ...|\n",
      "|2019-10-29 14:41:51|1189190616577531905|                @JimmyStacks Nice...|\n",
      "|2019-10-29 14:41:50|1189190611351437312|                RT @CultureCentra...|\n",
      "|2019-10-29 14:41:49|1189190607064850433|                RT @CultureCentra...|\n",
      "|2019-10-29 14:41:48|1189190603713593351|                Wett Tipp: Buducn...|\n",
      "|2019-10-29 14:41:48|1189190602589536259|                That’s so nasty l...|\n",
      "|2019-10-29 14:41:46|1189190594964283392|                RT @smarktodeath:...|\n",
      "|2019-10-29 14:41:45|1189190589100638208|                RT @ShockerManiac...|\n",
      "|2019-10-29 14:41:42|1189190579827048450|                @TermineRadio rem...|\n",
      "+-------------------+-------------------+------------------------------------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "raw = spark.sql(\"select * from tweets order by created_at desc\")\n",
    "raw.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get the count of tweets per 10 seconds\n",
    "tweet_count_df = tweets.groupBy(window(tweets.created_at, '10 seconds')).count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [],
   "source": [
    "query = (tweet_count_df\n",
    "        .writeStream\n",
    "        .format(\"memory\")\n",
    "        .queryName(\"window_count\")\n",
    "        .outputMode(\"complete\")\n",
    "        .start())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------+-----+\n",
      "|              window|count|\n",
      "+--------------------+-----+\n",
      "|[2019-10-23 16:05...|   21|\n",
      "|[2019-10-23 16:05...|   15|\n",
      "|[2019-10-23 16:05...|   11|\n",
      "|[2019-10-23 16:05...|   23|\n",
      "|[2019-10-23 16:04...|   15|\n",
      "|[2019-10-23 16:04...|   14|\n",
      "|[2019-10-23 16:04...|   16|\n",
      "|[2019-10-23 16:04...|   20|\n",
      "|[2019-10-23 16:04...|   15|\n",
      "|[2019-10-23 16:04...|   17|\n",
      "|[2019-10-23 16:03...|   13|\n",
      "|[2019-10-23 16:03...|   10|\n",
      "|[2019-10-23 16:03...|   15|\n",
      "|[2019-10-23 16:03...|   12|\n",
      "|[2019-10-23 16:03...|    5|\n",
      "|[2019-10-23 16:03...|   13|\n",
      "|[2019-10-23 16:02...|   13|\n",
      "|[2019-10-23 16:02...|   23|\n",
      "|[2019-10-23 16:02...|   22|\n",
      "|[2019-10-23 16:02...|   13|\n",
      "+--------------------+-----+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "raw = spark.sql(\"select * from window_count order by window desc\")\n",
    "raw.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
