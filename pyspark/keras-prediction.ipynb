{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install elephas -q"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql import SparkSession, DataFrame\n",
    "from pyspark.sql.functions import window, avg, count\n",
    "from pyspark.sql import types as T\n",
    "import pyspark.sql.functions as F\n",
    "from pyspark.sql.window import Window\n",
    "\n",
    "# from pyspark.mllib.linalg import Matrix, Matrices, Vectors, SparseVector, DenseVector, VectorUDT\n",
    "from pyspark.ml.linalg import Matrix, Matrices, Vectors, SparseVector, DenseVector, VectorUDT\n",
    "from pyspark.mllib.evaluation import RegressionMetrics\n",
    "from pyspark.ml.feature import VectorAssembler\n",
    "from pyspark.ml import Pipeline, Transformer\n",
    "from pyspark.ml.regression import LinearRegression\n",
    "\n",
    "from elephas.ml_model import ElephasEstimator, ElephasTransformer\n",
    "from elephas.spark_model import SparkMLlibModel\n",
    "\n",
    "import numpy as np\n",
    "\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense\n",
    "from keras import backend as K\n",
    "from keras import optimizers"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Create the spark session"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "spark = (SparkSession\n",
    "         .builder\n",
    "         .appName(\"Streaming\")\n",
    "         .config('spark.jars.packages', 'org.mongodb.spark:mongo-spark-connector_2.11:2.4.1,org.apache.spark:spark-sql-kafka-0-10_2.11:2.4.4')\n",
    "         .getOrCreate())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Read our processed time windows from mongo"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------+-----------------+-------------------+-------------------+--------------------+\n",
      "|n_tweets|            price|          sentiment|          timestamp|              window|\n",
      "+--------+-----------------+-------------------+-------------------+--------------------+\n",
      "|      24|9186.630000000001|0.17804166666666665|2019-11-03 14:44:00|[2019-11-03 14:34...|\n",
      "|      73|9183.607500000002|0.24112465753424658|2019-11-03 14:46:00|[2019-11-03 14:36...|\n",
      "|     126|9182.878333333334|0.25534523809523824|2019-11-03 14:48:00|[2019-11-03 14:38...|\n",
      "|     184|         9181.015| 0.2538385869565217|2019-11-03 14:50:00|[2019-11-03 14:40...|\n",
      "|     257|         9184.389|0.21677548638132285|2019-11-03 14:52:00|[2019-11-03 14:42...|\n",
      "|     311|         9186.677|0.21157234726688087|2019-11-03 14:54:00|[2019-11-03 14:44...|\n",
      "|     312|         9189.179|0.19037852564102548|2019-11-03 14:56:00|[2019-11-03 14:46...|\n",
      "|     316|           9192.1|0.17361044303797465|2019-11-03 14:58:00|[2019-11-03 14:48...|\n",
      "|     313|         9195.474|0.16318370607028743|2019-11-03 15:00:00|[2019-11-03 14:50...|\n",
      "|     385|         9193.544|0.15191168831168814|2019-11-03 15:02:00|[2019-11-03 14:52...|\n",
      "|     429|         9190.258|0.13599184149184138|2019-11-03 15:04:00|[2019-11-03 14:54...|\n",
      "|     473|9186.289999999999| 0.1452315010570823|2019-11-03 15:06:00|[2019-11-03 14:56...|\n",
      "|     504|         9186.683|0.14681289682539672|2019-11-03 15:08:00|[2019-11-03 14:58...|\n",
      "|     558|9190.850999999999| 0.1321562724014335|2019-11-03 15:10:00|[2019-11-03 15:00...|\n",
      "|     512|         9195.039|0.14367148437499985|2019-11-03 15:12:00|[2019-11-03 15:02...|\n",
      "|     474|          9200.66|0.15276476793248936|2019-11-03 15:14:00|[2019-11-03 15:04...|\n",
      "|     462|9207.261999999999|0.13937878787878777|2019-11-03 15:16:00|[2019-11-03 15:06...|\n",
      "|     456|         9207.818| 0.1348307017543858|2019-11-03 15:18:00|[2019-11-03 15:08...|\n",
      "|     471|         9205.565|0.14266518046709115|2019-11-03 15:20:00|[2019-11-03 15:10...|\n",
      "|     490|9203.279999999999|0.13861673469387736|2019-11-03 15:22:00|[2019-11-03 15:12...|\n",
      "+--------+-----------------+-------------------+-------------------+--------------------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df = (spark\n",
    "         .read\n",
    "         .format(\"mongo\")\n",
    "         .option(\"spark.mongodb.input.uri\", \"mongodb://165.22.199.122/processed.internal\")\n",
    "         .load()\n",
    "         .drop('_id')\n",
    "         .orderBy('window.end'))\n",
    "\n",
    "df.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Create a transformer to calculate the price difference and generate the y labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "class PriceDiffTransformer(Transformer):\n",
    "    \"\"\"\n",
    "    Custorm tranformer that calculates the price difference since the last time period\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self):\n",
    "        super(PriceDiffTransformer, self).__init__()\n",
    "        \n",
    "    def _transform(self, df: DataFrame) -> DataFrame:\n",
    "        # Define the window function\n",
    "        window = Window.partitionBy().orderBy('timestamp')\n",
    "\n",
    "        # Create a price lag of 1 window\n",
    "        df = df.withColumn('prev_price', F.lag(df.price).over(window))\n",
    "\n",
    "        # Calculate the price difference\n",
    "        df = df.withColumn('price_diff', df.price - df.prev_price)\n",
    "        \n",
    "        # Y label\n",
    "        df = df.withColumn('label', F.lag(df.price_diff, -1).over(window))\n",
    "\n",
    "        # Drop the previous price column\n",
    "        df = df.drop('prev_price', 'window')\n",
    "        \n",
    "        # Drop all nan values (first price)\n",
    "        df = df.na.drop()\n",
    "\n",
    "        return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "price_diff_transformer = PriceDiffTransformer()\n",
    "df_price_diff = price_diff_transformer.transform(df)\n",
    "df_price_diff.show(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Create a transformer to bring all the features to one array"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "class TimeTransformer(Transformer):\n",
    "    \"\"\"\n",
    "    A custom Transformer which transforms all values to timeseries. This is needed to input it into\n",
    "    the neural network\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self):\n",
    "        super(TimeTransformer, self).__init__()\n",
    "\n",
    "    def _transform(self, df: DataFrame) -> DataFrame:\n",
    "        \n",
    "        # Create the timeseries. Window 24 minutes and collect the list of variables needed\n",
    "        df_window = (df\n",
    "             .groupBy(F.window(df.timestamp, '24 minutes', '2 minutes'))\n",
    "             .agg(\n",
    "                 F.collect_list('price_diff'), \n",
    "                 F.collect_list('sentiment'), \n",
    "                 F.collect_list('n_tweets'),\n",
    "                 F.max('timestamp').alias('timestamp'),\n",
    "                 F.last('label').alias('label')))\n",
    "\n",
    "        # Concatenate all array columns\n",
    "        df_features = df_window.withColumn('features', \n",
    "                    F.concat(\n",
    "                        F.col('collect_list(price_diff)'), \n",
    "                        F.col('collect_list(sentiment)'),\n",
    "                        F.col('collect_list(n_tweets)')))\n",
    "\n",
    "        # Make sure all the values are there\n",
    "        df_features = df_features.where(F.size(F.col('features')) == 36)\n",
    "        \n",
    "        # Dropped the left over array columns\n",
    "        df_features = df_features.drop(\n",
    "            'window', \n",
    "            'collect_list(price_diff)', \n",
    "            'collect_list(sentiment)', \n",
    "            'collect_list(n_tweets)')\n",
    "\n",
    "        # Parse the features as vector instead of array (length need to be consistent)\n",
    "        list_to_vector_udf = F.udf(lambda l: Vectors.dense(l), VectorUDT())\n",
    "\n",
    "        df_features = df_features.select(\n",
    "            df_features[\"label\"], \n",
    "            df_features[\"timestamp\"], \n",
    "            list_to_vector_udf(df_features[\"features\"]).alias(\"features\"))\n",
    "\n",
    "        return df_features.orderBy('timestamp').drop('timestamp')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "time_transformer = TimeTransformer()\n",
    "df_time = time_transformer.transform(df_price_diff)\n",
    "df_time.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Example of the pipeline without the estimator in the end"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "price_diff_transformer = PriceDiffTransformer()\n",
    "time_transformer = TimeTransformer()\n",
    "\n",
    "transform_pipeline = Pipeline(stages=[price_diff_transformer, time_transformer]).fit(df)\n",
    "#df_transform = transform_pipeline.transform(df)\n",
    "#df_transform.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Linear regression test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "price_diff_transformer = PriceDiffTransformer()\n",
    "time_transformer = TimeTransformer()\n",
    "lr_estimator = LinearRegression(maxIter=10, regParam=0.3, elasticNetParam=0.8)\n",
    "\n",
    "lr_pipeline = Pipeline(stages=[price_diff_transformer, time_transformer, lr_estimator]).fit(df)\n",
    "df_lr = lr_pipeline.transform(df)\n",
    "df_lr.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pred_rdd = df_lr.rdd.map(lambda p: (p.prediction, p.label)).cache()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "metrics = RegressionMetrics(pred_rdd)\n",
    "metrics.rootMeanSquaredError"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Elephas prediction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [],
   "source": [
    "def root_mean_squared_error(y_true, y_pred):\n",
    "    return K.sqrt(K.mean(K.square(y_pred - y_true))) \n",
    "\n",
    "def build_model():\n",
    "    model = Sequential()\n",
    "    model.add(Dense(64, activation='relu', input_shape=(36,)))\n",
    "    model.add(Dense(32, activation='relu'))\n",
    "        \n",
    "    model.add(Dense(1))\n",
    "    model.compile(optimizer='adam', loss=root_mean_squared_error)\n",
    "    \n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ElephantEstimator(ElephasEstimator):\n",
    "    def _transform(self, df):\n",
    "        \n",
    "        features_col=self.getFeaturesCol()\n",
    "    \n",
    "        output_col = self.getOutputCol()\n",
    "\n",
    "        label_col = self.getLabelCol()\n",
    "\n",
    "        #output_col = \"prediction\"\n",
    "\n",
    "        #label_col = \"label\"\n",
    "\n",
    "        new_schema = copy.deepcopy(df.schema)\n",
    "\n",
    "        new_schema.add(StructField(output_col, DoubleType(), True))\n",
    "\n",
    "        rdd = df.rdd.coalesce(1)\n",
    "\n",
    "        features = np.asarray(\n",
    "\n",
    "            #rdd.map(lambda x: from_vector(x.features)).collect())\n",
    "            rdd.map(lambda x: from_vector(x[features_col])).collect())\n",
    "\n",
    "            # Note that we collect, since executing this on the rdd would require model serialization once again\n",
    "\n",
    "        #display(len(features[0]))\n",
    "\n",
    "        model = model_from_yaml(self.get_keras_model_config())\n",
    "\n",
    "        model.set_weights(self.weights.value)\n",
    "\n",
    "        #prediction=model.predict(features)\n",
    "        #display(prediction)\n",
    "\n",
    "        predictions = rdd.ctx.parallelize(model.predict(features)).coalesce(1)\n",
    "\n",
    "        #display(predictions.take(2))\n",
    "\n",
    "        predictions = predictions.map(lambda x: [float(x)])\n",
    "\n",
    "        #display(predictions.take(2))\n",
    "\n",
    "        predictions = predictions.map(lambda x: tuple(x))\n",
    "\n",
    "        display(rdd.zip(predictions).take(2))\n",
    "\n",
    "        results_rdd = rdd.zip(predictions).map(lambda x: x[0] + x[1])\n",
    "\n",
    "        # TODO: Zipping like this is very likely wrong\n",
    "\n",
    "        # results_rdd = rdd.zip(predictions).map(lambda pair: Row(features=to_vector(pair[0].features),\n",
    "\n",
    "        #                                        label=pair[0].label, prediction=float(pair[1])))\n",
    "\n",
    "\n",
    "        #display(results_rdd.take(2))\n",
    "\n",
    "\n",
    "        results_df = df.sql_ctx.createDataFrame(results_rdd, new_schema)\n",
    "\n",
    "        results_df = results_df.withColumn(\n",
    "\n",
    "            output_col, results_df[output_col].cast(DoubleType()))\n",
    "\n",
    "        results_df = results_df.withColumn(\n",
    "\n",
    "            label_col, results_df[label_col].cast(DoubleType()))\n",
    "\n",
    "        return results_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential_2\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "dense_4 (Dense)              (None, 64)                2368      \n",
      "_________________________________________________________________\n",
      "dense_5 (Dense)              (None, 32)                2080      \n",
      "_________________________________________________________________\n",
      "dense_6 (Dense)              (None, 1)                 33        \n",
      "=================================================================\n",
      "Total params: 4,481\n",
      "Trainable params: 4,481\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "keras_model = build_model()\n",
    "keras_model.load_weights('models/keras_weights.hdf5')\n",
    "keras_model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "ElephantEstimator_893e51a7f654"
      ]
     },
     "execution_count": 89,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "adam = optimizers.Adam(lr=0.01)\n",
    "opt_conf = optimizers.serialize(adam)\n",
    "\n",
    "# Initialize SparkML Estimator and set all relevant properties\n",
    "estimator = ElephantEstimator()\n",
    "estimator.setFeaturesCol('features')\n",
    "estimator.setLabelCol('label')\n",
    "estimator.set_keras_model_config(keras_model.to_yaml())\n",
    "estimator.set_categorical_labels(False)\n",
    "# estimator.set_nb_classes(nb_classes)\n",
    "estimator.set_num_workers(1)\n",
    "estimator.set_epochs(5) \n",
    "estimator.set_batch_size(128)\n",
    "estimator.set_verbosity(1)\n",
    "estimator.set_validation_split(0.15)\n",
    "estimator.set_optimizer_config(opt_conf)\n",
    "estimator.set_mode('synchronous')\n",
    "estimator.set_loss('mean_squared_error')\n",
    "estimator.set_metrics(['mean_squared_error'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "model = Pipeline(stages=[price_diff_transformer, time_transformer, estimator]).fit(df)\n",
    "# df_pred = model.transform(df)\n",
    "# df_pred.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Streaming test"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Twitter stream"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the timestamp format\n",
    "timestampFormat = \"dd-MM-yyyy HH:mm:ss\"\n",
    "\n",
    "# Create the schema of incoming data\n",
    "twitter_schema = T.StructType([\n",
    "    T.StructField('timestamp', T.TimestampType(), False),\n",
    "    T.StructField('text', T.StringType(), False),\n",
    "    T.StructField('sentiment', T.DoubleType(), False)\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Read kafka stream and subscribe to twitter topic\n",
    "twitter_stream = (spark.readStream\n",
    "          .format('kafka')\n",
    "          .option('kafka.bootstrap.servers', 'kafka:9092')\n",
    "          .option('startingOffsets', 'latest')\n",
    "          .option('subscribe', 'twitter')\n",
    "          .load()\n",
    "          .select(F.col(\"key\").cast(\"string\"), \\\n",
    "                  F.from_json(F.col(\"value\").cast(\"string\"), twitter_schema, \\\n",
    "                  { \"timestampFormat\": timestampFormat }).alias(\"value\")))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create streaming moving windows\n",
    "twitter_aggregation = (twitter_stream\n",
    "                     .select('value.*')\n",
    "                     .withWatermark('timestamp', '5 seconds')\n",
    "                     .groupBy(window('timestamp', '10 minutes', '2 minutes'))\n",
    "                     .agg(avg('sentiment').alias('sentiment'), count('timestamp').alias('n_tweets'))).select(F.col('window.end').alias('timestamp'), F.col('sentiment'), F.col('n_tweets'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "twitter_agg_stream = (twitter_aggregation\n",
    "    .writeStream\n",
    "    .outputMode(\"append\")\n",
    "    .format(\"console\")\n",
    "    .start())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'message': 'Stopped', 'isDataAvailable': False, 'isTriggerActive': False}"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "twitter_agg_stream.stop()\n",
    "twitter_agg_stream.status"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<pyspark.sql.streaming.StreamingQuery at 0x7f32ada1c828>"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Add the timestamp as key\n",
    "twitter_aggregation = twitter_aggregation.withColumn('key', F.col('timestamp'))\n",
    "\n",
    "# Send the data to kafka\n",
    "(twitter_aggregation\n",
    "    .selectExpr(\"CAST(key AS STRING) AS key\", \"to_json(struct(*)) AS value\")\n",
    "    .writeStream\n",
    "    .format(\"kafka\")\n",
    "    .option(\"kafka.bootstrap.servers\", \"kafka:9092\")\n",
    "    .option(\"topic\", \"twitter-agg\")\n",
    "    .option(\"checkpointLocation\", \"checkpoints/twitter-agg\")\n",
    "    .start())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Crypto stream"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the timestamp format\n",
    "timestampFormat = \"dd-MM-yyyy HH:mm:ss\"\n",
    "\n",
    "# Create the schema of incoming data\n",
    "crypto_schema = T.StructType([\n",
    "    T.StructField('timestamp', T.TimestampType(), False),\n",
    "    T.StructField('price', T.DoubleType(), False)\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Read kafka stream and subscribe to crypto topic\n",
    "crypto_stream = (spark.readStream\n",
    "          .format('kafka')\n",
    "          .option('kafka.bootstrap.servers', 'kafka:9092')\n",
    "          .option('startingOffsets', 'latest')\n",
    "          .option('subscribe', 'crypto')\n",
    "          .load()\n",
    "          .select(F.col(\"key\").cast(\"string\"), \\\n",
    "                  F.from_json(F.col(\"value\").cast(\"string\"), crypto_schema, \\\n",
    "                  { \"timestampFormat\": timestampFormat }).alias(\"value\")))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create streaming moving windows\n",
    "crypto_aggregation = (crypto_stream\n",
    "                     .select('value.*')\n",
    "                     .withWatermark('timestamp', '5 seconds')\n",
    "                     .groupBy(window('timestamp', '10 minutes', '2 minutes'))\n",
    "                     .agg(avg('price').alias('price'))).select(F.col('window.end').alias('timestamp'), F.col('price'))\n",
    "\n",
    "# Successfully ingested this stream"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<pyspark.sql.streaming.StreamingQuery at 0x7f32ada1cc50>"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Add the timestamp as key\n",
    "crypto_aggregation = crypto_aggregation.withColumn('key', F.col('timestamp'))\n",
    "\n",
    "# Send the data to kafka\n",
    "(crypto_aggregation\n",
    "    .selectExpr(\"CAST(key AS STRING) AS key\", \"to_json(struct(*)) AS value\")\n",
    "    .writeStream\n",
    "    .format(\"kafka\")\n",
    "    .option(\"kafka.bootstrap.servers\", \"kafka:9092\")\n",
    "    .option(\"topic\", \"crypto-agg\")\n",
    "    .option(\"checkpointLocation\", \"checkpoints/crypto-agg\")\n",
    "    .start())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Read the crypto aggregation stream"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- timestamp: timestamp (nullable = true)\n",
      " |-- price: double (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Create the schema of incoming aggregated crypto data\n",
    "crypto_agg_schema = T.StructType([\n",
    "    T.StructField('timestamp', T.TimestampType(), False),\n",
    "    T.StructField('price', T.DoubleType(), False)\n",
    "])\n",
    "\n",
    "# Read the crypto aggregation stream\n",
    "crypto_agg_stream = ((spark.readStream\n",
    "          .format('kafka')\n",
    "          .option('kafka.bootstrap.servers', 'kafka:9092')\n",
    "          .option('startingOffsets', 'latest')\n",
    "          .option('subscribe', 'crypto-agg')\n",
    "          .load()\n",
    "          .select(\n",
    "              F.col(\"key\").cast(\"string\"), \n",
    "              F.from_json(F.col(\"value\").cast(\"string\"), crypto_agg_schema).alias(\"value\")))\n",
    "                     .select('value.*'))\n",
    "\n",
    "crypto_agg_stream.printSchema()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Read the twitter aggregation stream"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- timestamp: timestamp (nullable = true)\n",
      " |-- sentiment: double (nullable = true)\n",
      " |-- n_tweets: integer (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Create the schema of incoming aggregated crypto data\n",
    "twitter_agg_schema = T.StructType([\n",
    "    T.StructField('timestamp', T.TimestampType(), False),\n",
    "    T.StructField('sentiment', T.DoubleType(), False),\n",
    "    T.StructField('n_tweets', T.IntegerType(), False)\n",
    "])\n",
    "\n",
    "# Read the twitter aggregation stream\n",
    "twitter_agg_stream = ((spark.readStream\n",
    "          .format('kafka')\n",
    "          .option('kafka.bootstrap.servers', 'kafka:9092')\n",
    "          .option('startingOffsets', 'latest')\n",
    "          .option('subscribe', 'twitter-agg')\n",
    "          .load()\n",
    "          .select(\n",
    "              F.col(\"key\").cast(\"string\"), \n",
    "              F.from_json(F.col(\"value\").cast(\"string\"), twitter_agg_schema).alias(\"value\")))\n",
    "                     .select('value.*'))\n",
    "\n",
    "twitter_agg_stream.printSchema()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Join the two streams"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "merged_stream = (crypto_agg_stream\n",
    "                    .join(twitter_agg_stream, 'timestamp')\n",
    "                    .withWatermark('timestamp', '1 seconds'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Write merged stream to memory\n",
    "stream_reader = (merged_stream\n",
    "                       .writeStream\n",
    "                       .queryName('merged_stream')\n",
    "                       .format('memory')\n",
    "                       .start())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "stream_reader.stop()\n",
    "stream_reader.status"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create dataframe for the merged stream\n",
    "merged_dataframe = spark.sql('select * from merged_stream')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create ML pipeline\n",
    "price_diff_transformer = PriceDiffTransformer()\n",
    "time_transformer = TimeTransformer()\n",
    "lr_estimator = LinearRegression(maxIter=10, regParam=0.3, elasticNetParam=0.8)\n",
    "\n",
    "lr_pipeline = Pipeline(stages=[price_diff_transformer, time_transformer, lr_estimator])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Fit pipeline on historic data (df = historic processed dataset from mongo database)\n",
    "lr_model = lr_pipeline.fit(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------+--------------------+--------------------+\n",
      "|               label|            features|          prediction|\n",
      "+--------------------+--------------------+--------------------+\n",
      "| 0.08521205187025771|[0.28216666666594...|-0.01304727055802...|\n",
      "|-0.03269565217578929|[-0.2347857142849...|0.004784843384880111|\n",
      "|   0.376347826088022|[0.15798571428513...|-0.11302319085549772|\n",
      "| 0.17443478261066048|[0.11616111111106...| 0.18359062315473934|\n",
      "|  0.2888695652181923|[0.12505994151979...|0.042201475695771706|\n",
      "+--------------------+--------------------+--------------------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Transform the dataframe w/ merged stream using the fitted pipeline\n",
    "tested_df = lr_model.transform(merged_dataframe)\n",
    "tested_df.show(5)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
