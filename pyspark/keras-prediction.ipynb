{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql import SparkSession, DataFrame\n",
    "from pyspark.sql.functions import window, avg, count\n",
    "from pyspark.sql import types as T\n",
    "import pyspark.sql.functions as F\n",
    "from pyspark.sql.window import Window\n",
    "\n",
    "# from pyspark.mllib.linalg import Matrix, Matrices, Vectors, SparseVector, DenseVector, VectorUDT\n",
    "from pyspark.ml.linalg import Matrix, Matrices, Vectors, SparseVector, DenseVector, VectorUDT\n",
    "from pyspark.mllib.evaluation import RegressionMetrics\n",
    "from pyspark.ml.feature import VectorAssembler\n",
    "from pyspark.ml import Pipeline, Transformer\n",
    "from pyspark.ml.regression import LinearRegression\n",
    "\n",
    "from elephas.ml_model import ElephasEstimator, ElephasTransformer\n",
    "from elephas.spark_model import SparkMLlibModel\n",
    "\n",
    "import numpy as np\n",
    "\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense\n",
    "from keras import backend as K\n",
    "from keras import optimizers"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Create the spark session"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "spark = (SparkSession\n",
    "         .builder\n",
    "         .appName(\"Twitter\")\n",
    "         .config('spark.jars.packages', 'org.mongodb.spark:mongo-spark-connector_2.11:2.4.1,org.apache.spark:spark-sql-kafka-0-10_2.11:2.4.4')\n",
    "         .getOrCreate())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Read our processed time windows from mongo"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------+-----------------+-------------------+-------------------+--------------------+\n",
      "|n_tweets|            price|          sentiment|          timestamp|              window|\n",
      "+--------+-----------------+-------------------+-------------------+--------------------+\n",
      "|      24|9186.630000000001|0.17804166666666665|2019-11-03 14:44:00|[2019-11-03 14:34...|\n",
      "|      73|9183.607500000002|0.24112465753424658|2019-11-03 14:46:00|[2019-11-03 14:36...|\n",
      "|     126|9182.878333333334|0.25534523809523824|2019-11-03 14:48:00|[2019-11-03 14:38...|\n",
      "|     184|         9181.015| 0.2538385869565217|2019-11-03 14:50:00|[2019-11-03 14:40...|\n",
      "|     257|         9184.389|0.21677548638132285|2019-11-03 14:52:00|[2019-11-03 14:42...|\n",
      "|     311|         9186.677|0.21157234726688087|2019-11-03 14:54:00|[2019-11-03 14:44...|\n",
      "|     312|         9189.179|0.19037852564102548|2019-11-03 14:56:00|[2019-11-03 14:46...|\n",
      "|     316|           9192.1|0.17361044303797465|2019-11-03 14:58:00|[2019-11-03 14:48...|\n",
      "|     313|         9195.474|0.16318370607028743|2019-11-03 15:00:00|[2019-11-03 14:50...|\n",
      "|     385|         9193.544|0.15191168831168814|2019-11-03 15:02:00|[2019-11-03 14:52...|\n",
      "|     429|         9190.258|0.13599184149184138|2019-11-03 15:04:00|[2019-11-03 14:54...|\n",
      "|     473|9186.289999999999| 0.1452315010570823|2019-11-03 15:06:00|[2019-11-03 14:56...|\n",
      "|     504|         9186.683|0.14681289682539672|2019-11-03 15:08:00|[2019-11-03 14:58...|\n",
      "|     558|9190.850999999999| 0.1321562724014335|2019-11-03 15:10:00|[2019-11-03 15:00...|\n",
      "|     512|         9195.039|0.14367148437499985|2019-11-03 15:12:00|[2019-11-03 15:02...|\n",
      "|     474|          9200.66|0.15276476793248936|2019-11-03 15:14:00|[2019-11-03 15:04...|\n",
      "|     462|9207.261999999999|0.13937878787878777|2019-11-03 15:16:00|[2019-11-03 15:06...|\n",
      "|     456|         9207.818| 0.1348307017543858|2019-11-03 15:18:00|[2019-11-03 15:08...|\n",
      "|     471|         9205.565|0.14266518046709115|2019-11-03 15:20:00|[2019-11-03 15:10...|\n",
      "|     490|9203.279999999999|0.13861673469387736|2019-11-03 15:22:00|[2019-11-03 15:12...|\n",
      "+--------+-----------------+-------------------+-------------------+--------------------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df = (spark\n",
    "         .read\n",
    "         .format(\"mongo\")\n",
    "         .option(\"spark.mongodb.input.uri\", \"mongodb://165.22.199.122/processed.internal\")\n",
    "         .load()\n",
    "         .drop('_id')\n",
    "         .orderBy('window.end'))\n",
    "\n",
    "df.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Create a transformer to calculate the price difference and generate the y labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "class PriceDiffTransformer(Transformer):\n",
    "    \"\"\"\n",
    "    Custorm tranformer that calculates the price difference since the last time period\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self):\n",
    "        super(PriceDiffTransformer, self).__init__()\n",
    "        \n",
    "    def _transform(self, df: DataFrame) -> DataFrame:\n",
    "        # Define the window function\n",
    "        window = Window.partitionBy().orderBy('window.start')\n",
    "\n",
    "        # Create a price lag of 1 window\n",
    "        df = df.withColumn('prev_price', F.lag(df.price).over(window))\n",
    "\n",
    "        # Calculate the price difference\n",
    "        df = df.withColumn('price_diff', df.price - df.prev_price)\n",
    "        \n",
    "        # Y label\n",
    "        df = df.withColumn('label', F.lag(df.price_diff, -1).over(window))\n",
    "\n",
    "        # Drop the previous price column\n",
    "        df = df.drop('prev_price', 'window')\n",
    "        \n",
    "        # Drop all nan values (first price)\n",
    "        df = df.na.drop()\n",
    "\n",
    "        return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------+-----------------+-------------------+-------------------+-------------------+-------------------+\n",
      "|n_tweets|            price|          sentiment|          timestamp|         price_diff|              label|\n",
      "+--------+-----------------+-------------------+-------------------+-------------------+-------------------+\n",
      "|      73|9183.607500000002|0.24112465753424658|2019-11-03 14:46:00| -3.022499999999127|-0.7291666666678793|\n",
      "|     126|9182.878333333334|0.25534523809523824|2019-11-03 14:48:00|-0.7291666666678793|-1.8633333333345945|\n",
      "|     184|         9181.015| 0.2538385869565217|2019-11-03 14:50:00|-1.8633333333345945| 3.3739999999997963|\n",
      "|     257|         9184.389|0.21677548638132285|2019-11-03 14:52:00| 3.3739999999997963| 2.2880000000004657|\n",
      "|     311|         9186.677|0.21157234726688087|2019-11-03 14:54:00| 2.2880000000004657| 2.5020000000004075|\n",
      "+--------+-----------------+-------------------+-------------------+-------------------+-------------------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "price_diff_transformer = PriceDiffTransformer()\n",
    "df_price_diff = price_diff_transformer.transform(df)\n",
    "df_price_diff.show(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Create a transformer to bring all the features to one array"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "class TimeTransformer(Transformer):\n",
    "    \"\"\"\n",
    "    A custom Transformer which transforms all values to timeseries. This is needed to input it into\n",
    "    the neural network\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self):\n",
    "        super(TimeTransformer, self).__init__()\n",
    "\n",
    "    def _transform(self, df: DataFrame) -> DataFrame:\n",
    "        \n",
    "        # Create the timeseries. Window 24 minutes and collect the list of variables needed\n",
    "        df_window = (df\n",
    "             .groupBy(F.window(df.timestamp, '24 minutes', '2 minutes'))\n",
    "             .agg(\n",
    "                 F.collect_list('price_diff'), \n",
    "                 F.collect_list('sentiment'), \n",
    "                 F.collect_list('n_tweets'),\n",
    "                 F.max('timestamp').alias('timestamp'),\n",
    "                 F.last('label').alias('label')))\n",
    "\n",
    "        # Concatenate all array columns\n",
    "        df_features = df_window.withColumn('features', \n",
    "                    F.concat(\n",
    "                        F.col('collect_list(price_diff)'), \n",
    "                        F.col('collect_list(sentiment)'),\n",
    "                        F.col('collect_list(n_tweets)')))\n",
    "\n",
    "        # Make sure all the values are there\n",
    "        df_features = df_features.where(F.size(F.col('features')) == 36)\n",
    "        \n",
    "        # Dropped the left over array columns\n",
    "        df_features = df_features.drop(\n",
    "            'window', \n",
    "            'collect_list(price_diff)', \n",
    "            'collect_list(sentiment)', \n",
    "            'collect_list(n_tweets)')\n",
    "\n",
    "        # Parse the features as vector instead of array (length need to be consistent)\n",
    "        list_to_vector_udf = F.udf(lambda l: Vectors.dense(l), VectorUDT())\n",
    "\n",
    "        df_features = df_features.select(\n",
    "            df_features[\"label\"], \n",
    "            df_features[\"timestamp\"], \n",
    "            list_to_vector_udf(df_features[\"features\"]).alias(\"features\"))\n",
    "\n",
    "        return df_features.orderBy('timestamp').drop('timestamp')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------------------+--------------------+\n",
      "|              label|            features|\n",
      "+-------------------+--------------------+\n",
      "|  4.167999999997846|[-3.0224999999991...|\n",
      "|  4.188000000001921|[-0.7291666666678...|\n",
      "|  5.620999999999185|[-1.8633333333345...|\n",
      "|  6.601999999998952|[3.37399999999979...|\n",
      "| 0.5560000000004948|[2.28800000000046...|\n",
      "| -2.252999999998792|[2.50200000000040...|\n",
      "|-2.2850000000016735|[2.92100000000027...|\n",
      "|-2.3690000000005966|[3.37399999999979...|\n",
      "|  2.293999999999869|[-1.9300000000002...|\n",
      "| 3.3920000000016444|[-3.2860000000000...|\n",
      "| 1.8870000000006257|[-3.9680000000007...|\n",
      "| 2.8659999999999854|[0.39300000000184...|\n",
      "| 3.3369999999995343|[4.16799999999784...|\n",
      "| -1.728000000000975|[4.18800000000192...|\n",
      "| -1.617999999998574|[5.62099999999918...|\n",
      "|-2.0610000000015134|[6.60199999999895...|\n",
      "|-2.6849999999994907|[0.55600000000049...|\n",
      "| -3.996999999999389|[-2.2529999999987...|\n",
      "| -4.450999999999112|[-2.2850000000016...|\n",
      "| -4.472000000001572|[-2.3690000000005...|\n",
      "+-------------------+--------------------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "time_transformer = TimeTransformer()\n",
    "df_time = time_transformer.transform(df_price_diff)\n",
    "df_time.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Example of the pipeline without the estimator in the end"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------------------+--------------------+\n",
      "|              label|            features|\n",
      "+-------------------+--------------------+\n",
      "|  4.167999999997846|[-3.0224999999991...|\n",
      "|  4.188000000001921|[-0.7291666666678...|\n",
      "|  5.620999999999185|[-1.8633333333345...|\n",
      "|  6.601999999998952|[3.37399999999979...|\n",
      "| 0.5560000000004948|[2.28800000000046...|\n",
      "| -2.252999999998792|[2.50200000000040...|\n",
      "|-2.2850000000016735|[2.92100000000027...|\n",
      "|-2.3690000000005966|[3.37399999999979...|\n",
      "|  2.293999999999869|[-1.9300000000002...|\n",
      "| 3.3920000000016444|[-3.2860000000000...|\n",
      "| 1.8870000000006257|[-3.9680000000007...|\n",
      "| 2.8659999999999854|[0.39300000000184...|\n",
      "| 3.3369999999995343|[4.16799999999784...|\n",
      "| -1.728000000000975|[4.18800000000192...|\n",
      "| -1.617999999998574|[5.62099999999918...|\n",
      "|-2.0610000000015134|[6.60199999999895...|\n",
      "|-2.6849999999994907|[0.55600000000049...|\n",
      "| -3.996999999999389|[-2.2529999999987...|\n",
      "| -4.450999999999112|[-2.2850000000016...|\n",
      "| -4.472000000001572|[-2.3690000000005...|\n",
      "+-------------------+--------------------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "price_diff_transformer = PriceDiffTransformer()\n",
    "time_transformer = TimeTransformer()\n",
    "\n",
    "model = Pipeline(stages=[price_diff_transformer, time_transformer]).fit(df)\n",
    "df_ml = model.transform(df)\n",
    "df_ml.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Linear regression test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------------------+--------------------+--------------------+\n",
      "|              label|            features|          prediction|\n",
      "+-------------------+--------------------+--------------------+\n",
      "|  4.167999999997846|[-3.0224999999991...| 0.20530550070523923|\n",
      "|  4.188000000001921|[-0.7291666666678...|  3.3623389446252427|\n",
      "|  5.620999999999185|[-1.8633333333345...|  3.4353698813804994|\n",
      "|  6.601999999998952|[3.37399999999979...|   4.327813524679513|\n",
      "| 0.5560000000004948|[2.28800000000046...|   4.732287874305415|\n",
      "| -2.252999999998792|[2.50200000000040...|-0.07643803759630463|\n",
      "|-2.2850000000016735|[2.92100000000027...|  -2.285360674683733|\n",
      "|-2.3690000000005966|[3.37399999999979...| -2.3487389008837147|\n",
      "|  2.293999999999869|[-1.9300000000002...| -2.1497957963860372|\n",
      "| 3.3920000000016444|[-3.2860000000000...|  1.7947690978912259|\n",
      "| 1.8870000000006257|[-3.9680000000007...|  2.7488014416828896|\n",
      "| 2.8659999999999854|[0.39300000000184...|  1.5426995218545216|\n",
      "| 3.3369999999995343|[4.16799999999784...|   2.078763051883532|\n",
      "| -1.728000000000975|[4.18800000000192...|   2.217470721663567|\n",
      "| -1.617999999998574|[5.62099999999918...| -1.6565266216450847|\n",
      "|-2.0610000000015134|[6.60199999999895...| -1.5371379143557489|\n",
      "|-2.6849999999994907|[0.55600000000049...| -1.8861584619962743|\n",
      "| -3.996999999999389|[-2.2529999999987...|  -2.161691936218637|\n",
      "| -4.450999999999112|[-2.2850000000016...| -2.9788307007600037|\n",
      "| -4.472000000001572|[-2.3690000000005...|  -3.301513084529251|\n",
      "+-------------------+--------------------+--------------------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "price_diff_transformer = PriceDiffTransformer()\n",
    "time_transformer = TimeTransformer()\n",
    "lr_estimator = LinearRegression(maxIter=10, regParam=0.3, elasticNetParam=0.8)\n",
    "\n",
    "lr_pipeline = Pipeline(stages=[price_diff_transformer, time_transformer, lr_estimator]).fit(df)\n",
    "df_lr = lr_pipeline.transform(df)\n",
    "df_lr.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [],
   "source": [
    "pred_rdd = df_lr.rdd.map(lambda p: ((float(p.prediction)), p.label)).cache()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "3.4540291933605607"
      ]
     },
     "execution_count": 57,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "metrics = RegressionMetrics(pred_rdd)\n",
    "metrics.rootMeanSquaredError"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Elephas prediction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 277,
   "metadata": {},
   "outputs": [],
   "source": [
    "def root_mean_squared_error(y_true, y_pred):\n",
    "    return K.sqrt(K.mean(K.square(y_pred - y_true))) \n",
    "\n",
    "def build_model():\n",
    "    model = Sequential()\n",
    "    model.add(Dense(64, activation='relu', input_shape=(36,)))\n",
    "    model.add(Dense(32, activation='relu'))\n",
    "        \n",
    "    model.add(Dense(1))\n",
    "    model.compile(optimizer='adam', loss=root_mean_squared_error)\n",
    "    \n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 278,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential_7\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "dense_19 (Dense)             (None, 64)                2368      \n",
      "_________________________________________________________________\n",
      "dense_20 (Dense)             (None, 32)                2080      \n",
      "_________________________________________________________________\n",
      "dense_21 (Dense)             (None, 1)                 33        \n",
      "=================================================================\n",
      "Total params: 4,481\n",
      "Trainable params: 4,481\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "keras_model = build_model()\n",
    "keras_model.load_weights('models/keras_weights.hdf5')\n",
    "keras_model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 279,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "ElephasEstimator_ebae0e9247e9"
      ]
     },
     "execution_count": 279,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "adam = optimizers.Adam(lr=0.01)\n",
    "opt_conf = optimizers.serialize(adam)\n",
    "\n",
    "# Initialize SparkML Estimator and set all relevant properties\n",
    "estimator = ElephasEstimator()\n",
    "estimator.setFeaturesCol('features')\n",
    "estimator.setLabelCol('label')\n",
    "estimator.set_keras_model_config(keras_model.to_yaml())\n",
    "estimator.set_categorical_labels(False)\n",
    "# estimator.set_nb_classes(nb_classes)\n",
    "estimator.set_num_workers(1)\n",
    "estimator.set_epochs(5) \n",
    "estimator.set_batch_size(128)\n",
    "estimator.set_verbosity(1)\n",
    "estimator.set_validation_split(0.15)\n",
    "estimator.set_optimizer_config(opt_conf)\n",
    "estimator.set_mode('synchronous')\n",
    "estimator.set_loss('mean_squared_error')\n",
    "estimator.set_metrics(['mean_squared_error'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "model = Pipeline(stages=[price_diff_transformer, time_transformer, estimator]).fit(df)\n",
    "df_pred = model.transform(df)\n",
    "df_pred.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Streaming test"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Twitter stream"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the timestamp format\n",
    "timestampFormat = \"dd-MM-yyyy HH:mm:ss\"\n",
    "\n",
    "# Create the schema of incoming data\n",
    "twitter_schema = StructType([\n",
    "    StructField('timestamp', TimestampType(), False),\n",
    "    StructField('text', StringType(), False),\n",
    "    StructField('sentiment', DoubleType(), False)\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Read kafka stream and subscribe to twitter topic\n",
    "twitter_df = (spark.readStream\n",
    "          .format('kafka')\n",
    "          .option('kafka.bootstrap.servers', 'kafka-1:9092')\n",
    "          .option('startingOffsets', 'latest')\n",
    "          .option('subscribe', 'twitter')\n",
    "          .load()\n",
    "          .select(col(\"key\").cast(\"string\"), \\\n",
    "                  from_json(col(\"value\").cast(\"string\"), twitter_schema, \\\n",
    "                  { \"timestampFormat\": timestampFormat }).alias(\"value\")))\n",
    "\n",
    "twitter_df.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "twitter = twitter_df.select('value.*')\n",
    "twitter.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "twitter_df_stream = (twitter\n",
    "         .writeStream\n",
    "         .queryName(\"twitter\")\n",
    "         .format(\"memory\")\n",
    "         .start())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "raw = spark.sql(\"select * from twitter\")\n",
    "raw.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Crypto stream"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the timestamp format\n",
    "timestampFormat = \"dd-MM-yyyy HH:mm:ss\"\n",
    "\n",
    "# Create the schema of incoming data\n",
    "crypto_schema = StructType([\n",
    "    StructField('timestamp', TimestampType(), False),\n",
    "    StructField('price', DoubleType(), False)\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Read kafka stream and subscribe to crypto topic\n",
    "crypto_df = (spark.readStream\n",
    "          .format('kafka')\n",
    "          .option('kafka.bootstrap.servers', 'kafka-1:9092')\n",
    "          .option('startingOffsets', 'latest')\n",
    "          .option('subscribe', 'crypto')\n",
    "          .load()\n",
    "          .select(col(\"key\").cast(\"string\"), \\\n",
    "                  from_json(col(\"value\").cast(\"string\"), crypto_schema, \\\n",
    "                  { \"timestampFormat\": timestampFormat }).alias(\"value\")))\n",
    "\n",
    "crypto_df.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "crypto = crypto_df.select('value.*')\n",
    "crypto.printSchema()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
