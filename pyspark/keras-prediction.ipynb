{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install elephas -q"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql import SparkSession, DataFrame\n",
    "from pyspark.sql.functions import window, avg, count\n",
    "from pyspark.sql import types as T\n",
    "import pyspark.sql.functions as F\n",
    "from pyspark.sql.window import Window\n",
    "\n",
    "# from pyspark.mllib.linalg import Matrix, Matrices, Vectors, SparseVector, DenseVector, VectorUDT\n",
    "from pyspark.ml.linalg import Matrix, Matrices, Vectors, SparseVector, DenseVector, VectorUDT\n",
    "from pyspark.mllib.evaluation import RegressionMetrics\n",
    "from pyspark.ml.feature import VectorAssembler\n",
    "from pyspark.ml import Pipeline, Transformer, PipelineModel\n",
    "from pyspark.ml.regression import LinearRegression\n",
    "\n",
    "from elephas.ml_model import ElephasEstimator, ElephasTransformer\n",
    "from elephas.spark_model import SparkMLlibModel\n",
    "\n",
    "import numpy as np\n",
    "\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense\n",
    "from keras import backend as K\n",
    "from keras import optimizers"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Create the spark session"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "spark = (SparkSession\n",
    "         .builder\n",
    "         .appName(\"Streaming\")\n",
    "         .config('spark.jars.packages', 'org.mongodb.spark:mongo-spark-connector_2.11:2.4.1,org.apache.spark:spark-sql-kafka-0-10_2.11:2.4.4')\n",
    "         .getOrCreate())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Read our processed time windows from mongo"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------+-----------------+-------------------+-------------------+--------------------+\n",
      "|n_tweets|            price|          sentiment|          timestamp|              window|\n",
      "+--------+-----------------+-------------------+-------------------+--------------------+\n",
      "|      24|9186.630000000001|0.17804166666666665|2019-11-03 14:44:00|[2019-11-03 14:34...|\n",
      "|      73|9183.607500000002|0.24112465753424658|2019-11-03 14:46:00|[2019-11-03 14:36...|\n",
      "|     126|9182.878333333334|0.25534523809523824|2019-11-03 14:48:00|[2019-11-03 14:38...|\n",
      "|     184|         9181.015| 0.2538385869565217|2019-11-03 14:50:00|[2019-11-03 14:40...|\n",
      "|     257|         9184.389|0.21677548638132285|2019-11-03 14:52:00|[2019-11-03 14:42...|\n",
      "|     311|         9186.677|0.21157234726688087|2019-11-03 14:54:00|[2019-11-03 14:44...|\n",
      "|     312|         9189.179|0.19037852564102548|2019-11-03 14:56:00|[2019-11-03 14:46...|\n",
      "|     316|           9192.1|0.17361044303797465|2019-11-03 14:58:00|[2019-11-03 14:48...|\n",
      "|     313|         9195.474|0.16318370607028743|2019-11-03 15:00:00|[2019-11-03 14:50...|\n",
      "|     385|         9193.544|0.15191168831168814|2019-11-03 15:02:00|[2019-11-03 14:52...|\n",
      "|     429|         9190.258|0.13599184149184138|2019-11-03 15:04:00|[2019-11-03 14:54...|\n",
      "|     473|9186.289999999999| 0.1452315010570823|2019-11-03 15:06:00|[2019-11-03 14:56...|\n",
      "|     504|         9186.683|0.14681289682539672|2019-11-03 15:08:00|[2019-11-03 14:58...|\n",
      "|     558|9190.850999999999| 0.1321562724014335|2019-11-03 15:10:00|[2019-11-03 15:00...|\n",
      "|     512|         9195.039|0.14367148437499985|2019-11-03 15:12:00|[2019-11-03 15:02...|\n",
      "|     474|          9200.66|0.15276476793248936|2019-11-03 15:14:00|[2019-11-03 15:04...|\n",
      "|     462|9207.261999999999|0.13937878787878777|2019-11-03 15:16:00|[2019-11-03 15:06...|\n",
      "|     456|         9207.818| 0.1348307017543858|2019-11-03 15:18:00|[2019-11-03 15:08...|\n",
      "|     471|         9205.565|0.14266518046709115|2019-11-03 15:20:00|[2019-11-03 15:10...|\n",
      "|     490|9203.279999999999|0.13861673469387736|2019-11-03 15:22:00|[2019-11-03 15:12...|\n",
      "+--------+-----------------+-------------------+-------------------+--------------------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df = (spark\n",
    "         .read\n",
    "         .format(\"mongo\")\n",
    "         .option(\"spark.mongodb.input.uri\", \"mongodb://165.22.199.122/processed.internal\")\n",
    "         .load()\n",
    "         .drop('_id')\n",
    "         .orderBy('window.end'))\n",
    "\n",
    "df.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Create a transformer to calculate the price difference and generate the y labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "class LabelTransformer(Transformer):\n",
    "    \"\"\"\n",
    "    Custorm tranformer that the label that needs to be predicted\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self):\n",
    "        super(LabelTransformer, self).__init__()\n",
    "        \n",
    "    def _transform(self, df: DataFrame) -> DataFrame:\n",
    "        \n",
    "        # Just return the dataframe if it is streaming, we cannot calculate the label\n",
    "        if df.isStreaming:\n",
    "            return df.withColumn('label', F.lin(0))\n",
    "        \n",
    "        # Define the window function\n",
    "        window = Window.partitionBy().orderBy('timestamp')\n",
    "\n",
    "        # Create a price lag of 1 window\n",
    "        df = df.withColumn('prev_price', F.lag(df.price).over(window))\n",
    "\n",
    "        # Calculate the price difference\n",
    "        df = df.withColumn('price_diff', df.price - df.prev_price)\n",
    "        \n",
    "        # Y label\n",
    "        df = df.withColumn('label', F.lag(df.price, -2).over(window))\n",
    "\n",
    "        # Drop the previous price column\n",
    "        df = df.drop('prev_price', 'window')\n",
    "        \n",
    "        # Drop all nan values (first price)\n",
    "        df = df.na.drop()\n",
    "\n",
    "        return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------+-----------------+-------------------+-------------------+-------------------+--------+\n",
      "|n_tweets|            price|          sentiment|          timestamp|         price_diff|   label|\n",
      "+--------+-----------------+-------------------+-------------------+-------------------+--------+\n",
      "|      73|9183.607500000002|0.24112465753424658|2019-11-03 14:46:00| -3.022499999999127|9181.015|\n",
      "|     126|9182.878333333334|0.25534523809523824|2019-11-03 14:48:00|-0.7291666666678793|9184.389|\n",
      "|     184|         9181.015| 0.2538385869565217|2019-11-03 14:50:00|-1.8633333333345945|9186.677|\n",
      "|     257|         9184.389|0.21677548638132285|2019-11-03 14:52:00| 3.3739999999997963|9189.179|\n",
      "|     311|         9186.677|0.21157234726688087|2019-11-03 14:54:00| 2.2880000000004657|  9192.1|\n",
      "+--------+-----------------+-------------------+-------------------+-------------------+--------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "label_transformer = LabelTransformer()\n",
    "df_label = label_transformer.transform(df)\n",
    "df_label.show(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Create a transformer to bring all the features to one array"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "class TimeTransformer(Transformer):\n",
    "    \"\"\"\n",
    "    A custom Transformer which transforms all values to timeseries. This is needed to input it into\n",
    "    the neural network\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, window_size, slide_size, feature_length):\n",
    "        super(TimeTransformer, self).__init__()\n",
    "        self.window_size = window_size\n",
    "        self.slide_size = slide_size\n",
    "        self.feature_length = feature_length\n",
    "\n",
    "    def _transform(self, df: DataFrame) -> DataFrame:\n",
    "        \n",
    "        # Create the timeseries. Window 24 minutes and collect the list of variables needed\n",
    "        df_window = (df\n",
    "             .groupBy(F.window(df.timestamp, self.window_size, self.slide_size))\n",
    "             .agg(\n",
    "                 F.collect_list('price'), \n",
    "                 F.collect_list('sentiment'), \n",
    "                 F.collect_list('n_tweets'),\n",
    "                 F.last('label').alias('label'),\n",
    "                 F.max('timestamp').alias('timestamp')))\n",
    "\n",
    "        # Concatenate all array columns\n",
    "        df_features = df_window.withColumn('features', \n",
    "                    F.concat(\n",
    "                        F.col('collect_list(price)'), \n",
    "                        F.col('collect_list(sentiment)'),\n",
    "                        F.col('collect_list(n_tweets)')))\n",
    "\n",
    "        # Make sure all the values are there\n",
    "        df_features = df_features.where(F.size(F.col('features')) == self.feature_length)\n",
    "        \n",
    "        # Dropped the left over array columns\n",
    "        df_features = df_features.drop(\n",
    "            'window', \n",
    "            'collect_list(price)', \n",
    "            'collect_list(sentiment)', \n",
    "            'collect_list(n_tweets)')\n",
    "\n",
    "        # Parse the features as vector instead of array (length need to be consistent)\n",
    "        list_to_vector_udf = F.udf(lambda l: Vectors.dense(l), VectorUDT())\n",
    "\n",
    "        df_features = df_features.select(\n",
    "            df_features[\"label\"], \n",
    "            list_to_vector_udf(df_features[\"features\"]).alias(\"features\"))\n",
    "\n",
    "        return df_features.drop('timestamp')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----------------+--------------------+\n",
      "|            label|            features|\n",
      "+-----------------+--------------------+\n",
      "|9190.783000000001|[9193.535,9191.23...|\n",
      "|         9177.135|[9181.14000000000...|\n",
      "|9166.708999999999|[9178.72499999999...|\n",
      "|          9205.12|[9188.31000000000...|\n",
      "|9219.877000000002|[9225.366,9225.54...|\n",
      "|9289.094000000001|[9276.361,9281.37...|\n",
      "|9507.947999999999|[9485.876,9506.06...|\n",
      "|         9338.444|[9349.09800000000...|\n",
      "|9334.369999999999|[9323.552,9326.79...|\n",
      "|         9334.596|[9334.103,9332.85...|\n",
      "|          9363.09|[9361.298,9362.43...|\n",
      "|9371.324999999999|[9385.72499999999...|\n",
      "|         9353.047|[9351.784,9352.34...|\n",
      "|9354.309000000001|[9352.392,9353.59...|\n",
      "|         9181.469|[9169.53799999999...|\n",
      "|9224.529999999999|[9223.59222222222...|\n",
      "|9012.176000000001|[9009.57000000000...|\n",
      "|         8773.946|[8788.408,8787.93...|\n",
      "|          8835.76|[8824.52600000000...|\n",
      "|         8811.962|[8815.41499999999...|\n",
      "+-----------------+--------------------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "time_transformer = TimeTransformer(\n",
    "    window_size='6 minutes', \n",
    "    slide_size='2 minutes', \n",
    "    feature_length=9)\n",
    "\n",
    "df_features = time_transformer.transform(df_label)\n",
    "df_features.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [],
   "source": [
    "lr_pipeline.write().overwrite().save('lr_pipeline')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lr_estimator = LinearRegression(maxIter=10, regParam=0.3, elasticNetParam=0.8)\n",
    "lr_pipeline = Pipeline(stages=[lr_estimator]).fit(df_features)\n",
    "\n",
    "# Save the pipeline\n",
    "lr_pipeline.write().overwrite().save('lr_pipeline')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----------------+--------------------+-----------------+\n",
      "|            label|            features|       prediction|\n",
      "+-----------------+--------------------+-----------------+\n",
      "|9190.783000000001|[9193.535,9191.23...|9190.584564115663|\n",
      "|         9177.135|[9181.14000000000...| 9179.07620051795|\n",
      "|9166.708999999999|[9178.72499999999...|9173.560828482081|\n",
      "|          9205.12|[9188.31000000000...|9191.914361492025|\n",
      "|9219.877000000002|[9225.366,9225.54...|9224.582859729486|\n",
      "|9289.094000000001|[9276.361,9281.37...|9281.463804854786|\n",
      "|9507.947999999999|[9485.876,9506.06...| 9497.13243990458|\n",
      "|         9338.444|[9349.09800000000...| 9344.04252864767|\n",
      "|9334.369999999999|[9323.552,9326.79...|9326.498846735196|\n",
      "|         9334.596|[9334.103,9332.85...|9332.112661362595|\n",
      "|          9363.09|[9361.298,9362.43...|9361.081552730007|\n",
      "|9371.324999999999|[9385.72499999999...|9380.393790586293|\n",
      "|         9353.047|[9351.784,9352.34...| 9352.19848674924|\n",
      "|9354.309000000001|[9352.392,9353.59...| 9353.42440421845|\n",
      "|         9181.469|[9169.53799999999...|9172.745414562216|\n",
      "|9224.529999999999|[9223.59222222222...| 9224.01931169643|\n",
      "|9012.176000000001|[9009.57000000000...|9007.391314628296|\n",
      "|         8773.946|[8788.408,8787.93...| 8787.84369117269|\n",
      "|          8835.76|[8824.52600000000...|8825.921957773367|\n",
      "|         8811.962|[8815.41499999999...|8814.929941492288|\n",
      "+-----------------+--------------------+-----------------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df_transformed = lr_pipeline.transform(df_features)\n",
    "df_transformed.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [],
   "source": [
    "pred_rdd = df_transformed.rdd.map(lambda p: (p.prediction, p.label)).cache()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "10.201432235528591"
      ]
     },
     "execution_count": 60,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "metrics = RegressionMetrics(pred_rdd)\n",
    "metrics.rootMeanSquaredError"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Elephas prediction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [],
   "source": [
    "def root_mean_squared_error(y_true, y_pred):\n",
    "    return K.sqrt(K.mean(K.square(y_pred - y_true))) \n",
    "\n",
    "def build_model():\n",
    "    model = Sequential()\n",
    "    model.add(Dense(64, activation='relu', input_shape=(36,)))\n",
    "    model.add(Dense(32, activation='relu'))\n",
    "        \n",
    "    model.add(Dense(1))\n",
    "    model.compile(optimizer='adam', loss=root_mean_squared_error)\n",
    "    \n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ElephantEstimator(ElephasEstimator):\n",
    "    def _transform(self, df):\n",
    "        \n",
    "        features_col=self.getFeaturesCol()\n",
    "    \n",
    "        output_col = self.getOutputCol()\n",
    "\n",
    "        label_col = self.getLabelCol()\n",
    "\n",
    "        #output_col = \"prediction\"\n",
    "\n",
    "        #label_col = \"label\"\n",
    "\n",
    "        new_schema = copy.deepcopy(df.schema)\n",
    "\n",
    "        new_schema.add(StructField(output_col, DoubleType(), True))\n",
    "\n",
    "        rdd = df.rdd.coalesce(1)\n",
    "\n",
    "        features = np.asarray(\n",
    "\n",
    "            #rdd.map(lambda x: from_vector(x.features)).collect())\n",
    "            rdd.map(lambda x: from_vector(x[features_col])).collect())\n",
    "\n",
    "            # Note that we collect, since executing this on the rdd would require model serialization once again\n",
    "\n",
    "        #display(len(features[0]))\n",
    "\n",
    "        model = model_from_yaml(self.get_keras_model_config())\n",
    "\n",
    "        model.set_weights(self.weights.value)\n",
    "\n",
    "        #prediction=model.predict(features)\n",
    "        #display(prediction)\n",
    "\n",
    "        predictions = rdd.ctx.parallelize(model.predict(features)).coalesce(1)\n",
    "\n",
    "        #display(predictions.take(2))\n",
    "\n",
    "        predictions = predictions.map(lambda x: [float(x)])\n",
    "\n",
    "        #display(predictions.take(2))\n",
    "\n",
    "        predictions = predictions.map(lambda x: tuple(x))\n",
    "\n",
    "        display(rdd.zip(predictions).take(2))\n",
    "\n",
    "        results_rdd = rdd.zip(predictions).map(lambda x: x[0] + x[1])\n",
    "\n",
    "        # TODO: Zipping like this is very likely wrong\n",
    "\n",
    "        # results_rdd = rdd.zip(predictions).map(lambda pair: Row(features=to_vector(pair[0].features),\n",
    "\n",
    "        #                                        label=pair[0].label, prediction=float(pair[1])))\n",
    "\n",
    "\n",
    "        #display(results_rdd.take(2))\n",
    "\n",
    "\n",
    "        results_df = df.sql_ctx.createDataFrame(results_rdd, new_schema)\n",
    "\n",
    "        results_df = results_df.withColumn(\n",
    "\n",
    "            output_col, results_df[output_col].cast(DoubleType()))\n",
    "\n",
    "        results_df = results_df.withColumn(\n",
    "\n",
    "            label_col, results_df[label_col].cast(DoubleType()))\n",
    "\n",
    "        return results_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential_2\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "dense_4 (Dense)              (None, 64)                2368      \n",
      "_________________________________________________________________\n",
      "dense_5 (Dense)              (None, 32)                2080      \n",
      "_________________________________________________________________\n",
      "dense_6 (Dense)              (None, 1)                 33        \n",
      "=================================================================\n",
      "Total params: 4,481\n",
      "Trainable params: 4,481\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "keras_model = build_model()\n",
    "keras_model.load_weights('models/keras_weights.hdf5')\n",
    "keras_model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "ElephantEstimator_893e51a7f654"
      ]
     },
     "execution_count": 89,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "adam = optimizers.Adam(lr=0.01)\n",
    "opt_conf = optimizers.serialize(adam)\n",
    "\n",
    "# Initialize SparkML Estimator and set all relevant properties\n",
    "estimator = ElephantEstimator()\n",
    "estimator.setFeaturesCol('features')\n",
    "estimator.setLabelCol('label')\n",
    "estimator.set_keras_model_config(keras_model.to_yaml())\n",
    "estimator.set_categorical_labels(False)\n",
    "# estimator.set_nb_classes(nb_classes)\n",
    "estimator.set_num_workers(1)\n",
    "estimator.set_epochs(5) \n",
    "estimator.set_batch_size(128)\n",
    "estimator.set_verbosity(1)\n",
    "estimator.set_validation_split(0.15)\n",
    "estimator.set_optimizer_config(opt_conf)\n",
    "estimator.set_mode('synchronous')\n",
    "estimator.set_loss('mean_squared_error')\n",
    "estimator.set_metrics(['mean_squared_error'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "model = Pipeline(stages=[price_diff_transformer, time_transformer, estimator]).fit(df)\n",
    "# df_pred = model.transform(df)\n",
    "# df_pred.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Streaming test"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Twitter stream"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the timestamp format\n",
    "timestampFormat = \"dd-MM-yyyy HH:mm:ss\"\n",
    "\n",
    "# Create the schema of incoming data\n",
    "twitter_schema = T.StructType([\n",
    "    T.StructField('timestamp', T.TimestampType(), False),\n",
    "    T.StructField('text', T.StringType(), False),\n",
    "    T.StructField('sentiment', T.DoubleType(), False)\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Read kafka stream and subscribe to twitter topic\n",
    "twitter_stream = (spark.readStream\n",
    "          .format('kafka')\n",
    "          .option('kafka.bootstrap.servers', 'kafka:9092')\n",
    "          .option('startingOffsets', 'latest')\n",
    "          .option('subscribe', 'twitter')\n",
    "          .load()\n",
    "          .select(F.col(\"key\").cast(\"string\"), \\\n",
    "                  F.from_json(F.col(\"value\").cast(\"string\"), twitter_schema, \\\n",
    "                  { \"timestampFormat\": timestampFormat }).alias(\"value\")))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create streaming moving windows\n",
    "twitter_aggregation = (twitter_stream\n",
    "                     .select('value.*')\n",
    "                     .withWatermark('timestamp', '10 seconds')\n",
    "                     .groupBy(window('timestamp', '10 minutes', '2 minutes'))\n",
    "                     .agg(avg('sentiment').alias('sentiment'), count('timestamp').alias('n_tweets'))).select(F.col('window.end').alias('timestamp'), F.col('sentiment'), F.col('n_tweets'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "twitter_agg_stream = (twitter_aggregation\n",
    "    .writeStream\n",
    "    .outputMode(\"append\")\n",
    "    .format(\"console\")\n",
    "    .start())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'message': 'Stopped', 'isDataAvailable': False, 'isTriggerActive': False}"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "twitter_agg_stream.stop()\n",
    "twitter_agg_stream.status"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<pyspark.sql.streaming.StreamingQuery at 0x7f6b3663dd68>"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Add the timestamp as key\n",
    "twitter_aggregation = twitter_aggregation.withColumn('key', F.col('timestamp'))\n",
    "\n",
    "# Send the data to kafka\n",
    "(twitter_aggregation\n",
    "    .selectExpr(\"CAST(key AS STRING) AS key\", \"to_json(struct(*)) AS value\")\n",
    "    .writeStream\n",
    "    .format(\"kafka\")\n",
    "    .option(\"kafka.bootstrap.servers\", \"kafka:9092\")\n",
    "    .option(\"topic\", \"twitter-agg\")\n",
    "    .option(\"checkpointLocation\", \"checkpoints/twitter-agg\")\n",
    "    .start())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Crypto stream"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the timestamp format\n",
    "timestampFormat = \"dd-MM-yyyy HH:mm:ss\"\n",
    "\n",
    "# Create the schema of incoming data\n",
    "crypto_schema = T.StructType([\n",
    "    T.StructField('timestamp', T.TimestampType(), False),\n",
    "    T.StructField('price', T.DoubleType(), False)\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Read kafka stream and subscribe to crypto topic\n",
    "crypto_stream = (spark.readStream\n",
    "          .format('kafka')\n",
    "          .option('kafka.bootstrap.servers', 'kafka:9092')\n",
    "          .option('startingOffsets', 'latest')\n",
    "          .option('subscribe', 'crypto')\n",
    "          .load()\n",
    "          .select(F.col(\"key\").cast(\"string\"), \\\n",
    "                  F.from_json(F.col(\"value\").cast(\"string\"), crypto_schema, \\\n",
    "                  { \"timestampFormat\": timestampFormat }).alias(\"value\")))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create streaming moving windows\n",
    "crypto_aggregation = (crypto_stream\n",
    "                     .select('value.*')\n",
    "                     .withWatermark('timestamp', '10 seconds')\n",
    "                     .groupBy(window('timestamp', '10 minutes', '2 minutes'))\n",
    "                     .agg(avg('price').alias('price'))).select(F.col('window.end').alias('timestamp'), F.col('price'))\n",
    "\n",
    "# Successfully ingested this stream"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<pyspark.sql.streaming.StreamingQuery at 0x7f6b36644a20>"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Add the timestamp as key\n",
    "crypto_aggregation = crypto_aggregation.withColumn('key', F.col('timestamp'))\n",
    "\n",
    "# Send the data to kafka\n",
    "(crypto_aggregation\n",
    "    .selectExpr(\"CAST(key AS STRING) AS key\", \"to_json(struct(*)) AS value\")\n",
    "    .writeStream\n",
    "    .format(\"kafka\")\n",
    "    .option(\"kafka.bootstrap.servers\", \"kafka:9092\")\n",
    "    .option(\"topic\", \"crypto-agg\")\n",
    "    .option(\"checkpointLocation\", \"checkpoints/crypto-agg\")\n",
    "    .start())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Read the crypto aggregation stream"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- timestamp: timestamp (nullable = true)\n",
      " |-- price: double (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Create the schema of incoming aggregated crypto data\n",
    "crypto_agg_schema = T.StructType([\n",
    "    T.StructField('timestamp', T.TimestampType(), False),\n",
    "    T.StructField('price', T.DoubleType(), False)\n",
    "])\n",
    "\n",
    "# Read the crypto aggregation stream\n",
    "crypto_agg_stream = ((spark.readStream\n",
    "          .format('kafka')\n",
    "          .option('kafka.bootstrap.servers', 'kafka:9092')\n",
    "          .option('startingOffsets', 'latest')\n",
    "          .option('subscribe', 'crypto-agg')\n",
    "          .load()\n",
    "          .select(\n",
    "              F.col(\"key\").cast(\"string\"), \n",
    "              F.from_json(F.col(\"value\").cast(\"string\"), crypto_agg_schema).alias(\"value\")))\n",
    "                     .select('value.*'))\n",
    "\n",
    "crypto_agg_stream.printSchema()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Read the twitter aggregation stream"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- timestamp: timestamp (nullable = true)\n",
      " |-- sentiment: double (nullable = true)\n",
      " |-- n_tweets: integer (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Create the schema of incoming aggregated crypto data\n",
    "twitter_agg_schema = T.StructType([\n",
    "    T.StructField('timestamp', T.TimestampType(), False),\n",
    "    T.StructField('sentiment', T.DoubleType(), False),\n",
    "    T.StructField('n_tweets', T.IntegerType(), False)\n",
    "])\n",
    "\n",
    "# Read the twitter aggregation stream\n",
    "twitter_agg_stream = ((spark.readStream\n",
    "          .format('kafka')\n",
    "          .option('kafka.bootstrap.servers', 'kafka:9092')\n",
    "          .option('startingOffsets', 'latest')\n",
    "          .option('subscribe', 'twitter-agg')\n",
    "          .load()\n",
    "          .select(\n",
    "              F.col(\"key\").cast(\"string\"), \n",
    "              F.from_json(F.col(\"value\").cast(\"string\"), twitter_agg_schema).alias(\"value\")))\n",
    "                     .select('value.*'))\n",
    "\n",
    "twitter_agg_stream.printSchema()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Join the two streams"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "merged_stream = (crypto_agg_stream\n",
    "                    .join(twitter_agg_stream, 'timestamp')\n",
    "                    .withWatermark('timestamp', '10 seconds'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Write merged stream to memory\n",
    "merged_reader = (df_lr_pred\n",
    "                       .writeStream\n",
    "                       .queryName('merged_streams')\n",
    "                       .format('memory')\n",
    "                       .start())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'message': 'Stopped', 'isDataAvailable': False, 'isTriggerActive': False}"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "merged_reader.stop()\n",
    "merged_reader.status"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "20:50:00\n",
    "        7394.958728813559,\n",
    "        7395.494237288136,\n",
    "        7395.11153846154"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------------------+-----------------------------------------------------------------------------------------------------------------------------------+------------------+\n",
      "|timestamp          |features                                                                                                                           |prediction        |\n",
      "+-------------------+-----------------------------------------------------------------------------------------------------------------------------------+------------------+\n",
      "|2019-11-18 20:50:00|[7394.958728813559,7395.494237288136,7395.11153846154,0.11442416851441242,0.12960817307692307,0.1496274752475247,451.0,416.0,404.0]|7397.882370153243 |\n",
      "|2019-11-18 20:52:00|[7395.494237288136,7395.11153846154,7394.740000000001,0.12960817307692307,0.1496274752475247,0.1415411330049261,416.0,404.0,406.0] |7397.814034418346 |\n",
      "|2019-11-18 20:54:00|[7395.11153846154,7394.740000000001,7395.55264957265,0.1496274752475247,0.1415411330049261,0.14015862944162438,404.0,406.0,394.0]  |7397.78277284018  |\n",
      "|2019-11-18 20:56:00|[7394.740000000001,7395.55264957265,7396.703220338983,0.1415411330049261,0.14015862944162438,0.1143621761658031,406.0,394.0,386.0] |7398.2828331917335|\n",
      "+-------------------+-----------------------------------------------------------------------------------------------------------------------------------+------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "spark.sql('select * from merged_streams').show(truncate=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "merged_stream.isStreaming"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "lr_prediction_pipeline = PipelineModel.load('lr_pipeline')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "time_transformer = TimeTransformer(\n",
    "    window_size='6 minutes', \n",
    "    slide_size='2 minutes', \n",
    "    feature_length=9)\n",
    "\n",
    "df_features = time_transformer.transform(merged_stream)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 61,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_features.isStreaming"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_lr_pred = lr_prediction_pipeline.transform(df_features)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 63,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_lr_pred.isStreaming"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- timestamp: timestamp (nullable = true)\n",
      " |-- features: vector (nullable = true)\n",
      " |-- pred_timestamp: timestamp (nullable = true)\n",
      " |-- prediction: double (nullable = false)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df_lr_pred.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Write merged stream to memory\n",
    "stream_readers = (df_lr_pred\n",
    "                       .writeStream\n",
    "                       .queryName('merged_stream')\n",
    "                       .format('console')\n",
    "                       .start())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'message': 'Stopped', 'isDataAvailable': False, 'isTriggerActive': False}"
      ]
     },
     "execution_count": 65,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "stream_readers.stop()\n",
    "stream_readers.status"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [],
   "source": [
    "class TimeTransformer(Transformer):\n",
    "    \"\"\"\n",
    "    A custom Transformer which transforms all values to timeseries. This is needed to input it into\n",
    "    the neural network\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, window_size, slide_size, feature_length):\n",
    "        super(TimeTransformer, self).__init__()\n",
    "        self.window_size = window_size\n",
    "        self.slide_size = slide_size\n",
    "        self.feature_length = feature_length\n",
    "\n",
    "    def _transform(self, df: DataFrame) -> DataFrame:\n",
    "        \n",
    "        # Create the timeseries. Window 24 minutes and collect the list of variables needed\n",
    "        df_window = (df\n",
    "             .groupBy(F.window(df.timestamp, self.window_size, self.slide_size))\n",
    "             .agg(\n",
    "                 F.collect_list('price'), \n",
    "                 F.collect_list('sentiment'), \n",
    "                 F.collect_list('n_tweets'),\n",
    "                 F.max('timestamp').alias('timestamp')))\n",
    "\n",
    "        # Concatenate all array columns\n",
    "        df_features = df_window.withColumn('features', \n",
    "                    F.concat(\n",
    "                        F.col('collect_list(price)'), \n",
    "                        F.col('collect_list(sentiment)'),\n",
    "                        F.col('collect_list(n_tweets)')))\n",
    "\n",
    "        # Make sure all the values are there\n",
    "        df_features = df_features.where(F.size(F.col('features')) == self.feature_length)\n",
    "\n",
    "        # Parse the features as vector instead of array (length need to be consistent)\n",
    "        list_to_vector_udf = F.udf(lambda l: Vectors.dense(l), VectorUDT())\n",
    "\n",
    "        df_features = df_features.select(\n",
    "            df_features[\"timestamp\"],\n",
    "            list_to_vector_udf(df_features[\"features\"]).alias(\"features\"))\n",
    "        \n",
    "        # Add the time of the bitcoin price prediction\n",
    "        df_features = df_features.withColumn('pred_timestamp', (df_features.timestamp + F.expr('INTERVAL 4 MINUTES')))\n",
    "\n",
    "        return df_features"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
